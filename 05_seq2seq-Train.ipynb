{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder And Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. GPU测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2019-12-08 18:21:01,185 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2019-12-08 18:21:01,186 : DEBUG : Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.494 seconds.\n",
      "2019-12-08 18:21:01,679 : DEBUG : Loading model cost 0.494 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-12-08 18:21:01,680 : DEBUG : Prefix dict has been built succesfully.\n",
      "2019-12-08 18:21:02,362 : WARNING : Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "2019-12-08 18:21:02,377 : WARNING : Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "2019-12-08 18:21:02,386 : WARNING : Limited tf.summary API due to missing TensorBoard installation.\n",
      "2019-12-08 18:21:02,510 : WARNING : Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('/home/roger/kaikeba/03_lecture/code')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from utils.data_loader import build_dataset,load_dataset,preprocess_sentence,load_test_dataset\n",
    "from utils.wv_loader import load_embedding_matrix,load_vocab\n",
    "from utils.config import *\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "from utils.gpu_utils import config_gpu\n",
    "config_gpu()\n",
    "import tensorflow as tf\n",
    "from utils.plot_utils import plot_attention\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from seq2seq_tf2.batcher import train_batch_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build_dataset(train_data_path,test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"embed_size\"] = 500\n",
    "params[\"enc_units\"] = 512\n",
    "params[\"attn_units\"] = 512\n",
    "params[\"dec_units\"] = 512\n",
    "params[\"batch_size\"] = 64\n",
    "params[\"epochs\"] = 5\n",
    "params[\"max_enc_len\"] = 200\n",
    "params[\"max_dec_len\"] = 41\n",
    "params[\"embedding_dim\"] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, steps_per_epoch = train_batch_generator(batch_size=64,\n",
    "                                                 max_enc_len=params[\"max_enc_len\"],\n",
    "                                                 max_dec_len=params[\"max_dec_len\"])\n",
    "test_X = load_test_dataset(params[\"max_dec_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 加载vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,reverse_vocab=load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 加载预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=load_embedding_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 基本参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"embed_size\"] = 500\n",
    "params[\"enc_units\"] = units\n",
    "params[\"attn_units\"] = units\n",
    "params[\"dec_units\"] = units\n",
    "params[\"batch_size\"] = 64\n",
    "params[\"epochs\"] = 5\n",
    "params[\"max_enc_len\"] = 200\n",
    "params[\"max_dec_len\"] = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 构建Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_tf2.seq2seq_model import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 读取训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import checkpoint_dir,checkpoint_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(Seq2Seq=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "nuk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    nuk_mask = tf.math.equal(real, nuk_index)\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,nuk_mask))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # 逐个预测序列\n",
    "        predictions, _ = model(dec_input, dec_hidden, enc_output, targ)\n",
    "        \n",
    "        batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "\n",
    "        variables = model.encoder.trainable_variables + model.decoder.trainable_variables+ model.attention.trainable_variables\n",
    "    \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.7875\n",
      "Epoch 1 Batch 1 Loss 1.7876\n",
      "Epoch 1 Batch 2 Loss 1.7762\n",
      "Epoch 1 Batch 3 Loss 1.9339\n",
      "Epoch 1 Batch 4 Loss 1.8832\n",
      "Epoch 1 Batch 5 Loss 1.8522\n",
      "Epoch 1 Batch 6 Loss 1.6765\n",
      "Epoch 1 Batch 7 Loss 2.0191\n",
      "Epoch 1 Batch 8 Loss 1.7543\n",
      "Epoch 1 Batch 9 Loss 1.9231\n",
      "Epoch 1 Batch 10 Loss 1.5854\n",
      "Epoch 1 Batch 11 Loss 1.6374\n",
      "Epoch 1 Batch 12 Loss 1.9204\n",
      "Epoch 1 Batch 13 Loss 1.8724\n",
      "Epoch 1 Batch 14 Loss 1.5321\n",
      "Epoch 1 Batch 15 Loss 1.7678\n",
      "Epoch 1 Batch 16 Loss 1.7474\n",
      "Epoch 1 Batch 17 Loss 1.5806\n",
      "Epoch 1 Batch 18 Loss 1.5105\n",
      "Epoch 1 Batch 19 Loss 1.8581\n",
      "Epoch 1 Batch 20 Loss 1.8831\n",
      "Epoch 1 Batch 21 Loss 1.9312\n",
      "Epoch 1 Batch 22 Loss 1.7766\n",
      "Epoch 1 Batch 23 Loss 1.7456\n",
      "Epoch 1 Batch 24 Loss 2.0053\n",
      "Epoch 1 Batch 25 Loss 1.8021\n",
      "Epoch 1 Batch 26 Loss 1.5357\n",
      "Epoch 1 Batch 27 Loss 2.0714\n",
      "Epoch 1 Batch 28 Loss 1.7755\n",
      "Epoch 1 Batch 29 Loss 1.9026\n",
      "Epoch 1 Batch 30 Loss 1.7643\n",
      "Epoch 1 Batch 31 Loss 1.7572\n",
      "Epoch 1 Batch 32 Loss 1.8468\n",
      "Epoch 1 Batch 33 Loss 1.8379\n",
      "Epoch 1 Batch 34 Loss 2.0022\n",
      "Epoch 1 Batch 35 Loss 1.8934\n",
      "Epoch 1 Batch 36 Loss 1.6506\n",
      "Epoch 1 Batch 37 Loss 1.7552\n",
      "Epoch 1 Batch 38 Loss 1.8894\n",
      "Epoch 1 Batch 39 Loss 1.7988\n",
      "Epoch 1 Batch 40 Loss 1.6908\n",
      "Epoch 1 Batch 41 Loss 1.8349\n",
      "Epoch 1 Batch 42 Loss 1.7684\n",
      "Epoch 1 Batch 43 Loss 1.8583\n",
      "Epoch 1 Batch 44 Loss 1.9223\n",
      "Epoch 1 Batch 45 Loss 2.0566\n",
      "Epoch 1 Batch 46 Loss 1.8093\n",
      "Epoch 1 Batch 47 Loss 1.6586\n",
      "Epoch 1 Batch 48 Loss 1.7859\n",
      "Epoch 1 Batch 49 Loss 1.9627\n",
      "Epoch 1 Batch 50 Loss 1.9882\n",
      "Epoch 1 Batch 51 Loss 1.7105\n",
      "Epoch 1 Batch 52 Loss 1.7519\n",
      "Epoch 1 Batch 53 Loss 1.7235\n",
      "Epoch 1 Batch 54 Loss 1.9994\n",
      "Epoch 1 Batch 55 Loss 2.0417\n",
      "Epoch 1 Batch 56 Loss 1.8584\n",
      "Epoch 1 Batch 57 Loss 1.6241\n",
      "Epoch 1 Batch 58 Loss 1.6549\n",
      "Epoch 1 Batch 59 Loss 1.7073\n",
      "Epoch 1 Batch 60 Loss 1.7285\n",
      "Epoch 1 Batch 61 Loss 1.7289\n",
      "Epoch 1 Batch 62 Loss 1.7188\n",
      "Epoch 1 Batch 63 Loss 1.7622\n",
      "Epoch 1 Batch 64 Loss 1.6105\n",
      "Epoch 1 Batch 65 Loss 1.6368\n",
      "Epoch 1 Batch 66 Loss 1.9108\n",
      "Epoch 1 Batch 67 Loss 1.9111\n",
      "Epoch 1 Batch 68 Loss 1.8132\n",
      "Epoch 1 Batch 69 Loss 1.9341\n",
      "Epoch 1 Batch 70 Loss 1.7211\n",
      "Epoch 1 Batch 71 Loss 1.6463\n",
      "Epoch 1 Batch 72 Loss 1.7004\n",
      "Epoch 1 Batch 73 Loss 1.7754\n",
      "Epoch 1 Batch 74 Loss 1.8210\n",
      "Epoch 1 Batch 75 Loss 1.7012\n",
      "Epoch 1 Batch 76 Loss 1.6737\n",
      "Epoch 1 Batch 77 Loss 1.8257\n",
      "Epoch 1 Batch 78 Loss 1.8342\n",
      "Epoch 1 Batch 79 Loss 2.1533\n",
      "Epoch 1 Batch 80 Loss 1.5895\n",
      "Epoch 1 Batch 81 Loss 1.8884\n",
      "Epoch 1 Batch 82 Loss 1.8116\n",
      "Epoch 1 Batch 83 Loss 2.0723\n",
      "Epoch 1 Batch 84 Loss 1.7110\n",
      "Epoch 1 Batch 85 Loss 1.8295\n",
      "Epoch 1 Batch 86 Loss 1.7073\n",
      "Epoch 1 Batch 87 Loss 1.7234\n",
      "Epoch 1 Batch 88 Loss 1.7627\n",
      "Epoch 1 Batch 89 Loss 1.7756\n",
      "Epoch 1 Batch 90 Loss 1.6949\n",
      "Epoch 1 Batch 91 Loss 1.4879\n",
      "Epoch 1 Batch 92 Loss 1.8624\n",
      "Epoch 1 Batch 93 Loss 1.9117\n",
      "Epoch 1 Batch 94 Loss 1.5836\n",
      "Epoch 1 Batch 95 Loss 1.7372\n",
      "Epoch 1 Batch 96 Loss 1.8704\n",
      "Epoch 1 Batch 97 Loss 1.8123\n",
      "Epoch 1 Batch 98 Loss 1.7820\n",
      "Epoch 1 Batch 99 Loss 1.7294\n",
      "Epoch 1 Batch 100 Loss 1.6586\n",
      "Epoch 1 Batch 101 Loss 1.6197\n",
      "Epoch 1 Batch 102 Loss 1.6364\n",
      "Epoch 1 Batch 103 Loss 2.0131\n",
      "Epoch 1 Batch 104 Loss 1.9293\n",
      "Epoch 1 Batch 105 Loss 1.7071\n",
      "Epoch 1 Batch 106 Loss 1.6586\n",
      "Epoch 1 Batch 107 Loss 1.6216\n",
      "Epoch 1 Batch 108 Loss 1.5787\n",
      "Epoch 1 Batch 109 Loss 1.7692\n",
      "Epoch 1 Batch 110 Loss 1.9331\n",
      "Epoch 1 Batch 111 Loss 1.8293\n",
      "Epoch 1 Batch 112 Loss 1.7966\n",
      "Epoch 1 Batch 113 Loss 2.0040\n",
      "Epoch 1 Batch 114 Loss 2.0605\n",
      "Epoch 1 Batch 115 Loss 1.6765\n",
      "Epoch 1 Batch 116 Loss 1.8269\n",
      "Epoch 1 Batch 117 Loss 1.7596\n",
      "Epoch 1 Batch 118 Loss 1.7843\n",
      "Epoch 1 Batch 119 Loss 1.8065\n",
      "Epoch 1 Batch 120 Loss 1.8952\n",
      "Epoch 1 Batch 121 Loss 1.8082\n",
      "Epoch 1 Batch 122 Loss 1.6956\n",
      "Epoch 1 Batch 123 Loss 1.6643\n",
      "Epoch 1 Batch 124 Loss 1.5492\n",
      "Epoch 1 Batch 125 Loss 1.6669\n",
      "Epoch 1 Batch 126 Loss 1.6378\n",
      "Epoch 1 Batch 127 Loss 1.8667\n",
      "Epoch 1 Batch 128 Loss 1.9387\n",
      "Epoch 1 Batch 129 Loss 1.6520\n",
      "Epoch 1 Batch 130 Loss 1.7517\n",
      "Epoch 1 Batch 131 Loss 1.8752\n",
      "Epoch 1 Batch 132 Loss 1.7896\n",
      "Epoch 1 Batch 133 Loss 1.5105\n",
      "Epoch 1 Batch 134 Loss 1.8046\n",
      "Epoch 1 Batch 135 Loss 1.8504\n",
      "Epoch 1 Batch 136 Loss 1.6561\n",
      "Epoch 1 Batch 137 Loss 1.7688\n",
      "Epoch 1 Batch 138 Loss 1.8973\n",
      "Epoch 1 Batch 139 Loss 1.5094\n",
      "Epoch 1 Batch 140 Loss 1.7248\n",
      "Epoch 1 Batch 141 Loss 1.6963\n",
      "Epoch 1 Batch 142 Loss 1.8887\n",
      "Epoch 1 Batch 143 Loss 1.6585\n",
      "Epoch 1 Batch 144 Loss 1.8907\n",
      "Epoch 1 Batch 145 Loss 2.1588\n",
      "Epoch 1 Batch 146 Loss 1.6668\n",
      "Epoch 1 Batch 147 Loss 1.9905\n",
      "Epoch 1 Batch 148 Loss 1.5225\n",
      "Epoch 1 Batch 149 Loss 1.7261\n",
      "Epoch 1 Batch 150 Loss 2.0439\n",
      "Epoch 1 Batch 151 Loss 1.8003\n",
      "Epoch 1 Batch 152 Loss 1.7998\n",
      "Epoch 1 Batch 153 Loss 1.7448\n",
      "Epoch 1 Batch 154 Loss 1.8643\n",
      "Epoch 1 Batch 155 Loss 1.6941\n",
      "Epoch 1 Batch 156 Loss 1.9678\n",
      "Epoch 1 Batch 157 Loss 1.7356\n",
      "Epoch 1 Batch 158 Loss 1.7625\n",
      "Epoch 1 Batch 159 Loss 1.9045\n",
      "Epoch 1 Batch 160 Loss 1.7555\n",
      "Epoch 1 Batch 161 Loss 1.6334\n",
      "Epoch 1 Batch 162 Loss 1.8875\n",
      "Epoch 1 Batch 163 Loss 1.5921\n",
      "Epoch 1 Batch 164 Loss 1.7489\n",
      "Epoch 1 Batch 165 Loss 1.8143\n",
      "Epoch 1 Batch 166 Loss 1.9233\n",
      "Epoch 1 Batch 167 Loss 1.9637\n",
      "Epoch 1 Batch 168 Loss 1.6523\n",
      "Epoch 1 Batch 169 Loss 1.4433\n",
      "Epoch 1 Batch 170 Loss 1.7767\n",
      "Epoch 1 Batch 171 Loss 1.8518\n",
      "Epoch 1 Batch 172 Loss 1.9133\n",
      "Epoch 1 Batch 173 Loss 1.8029\n",
      "Epoch 1 Batch 174 Loss 1.5887\n",
      "Epoch 1 Batch 175 Loss 1.7827\n",
      "Epoch 1 Batch 176 Loss 1.8033\n",
      "Epoch 1 Batch 177 Loss 1.6720\n",
      "Epoch 1 Batch 178 Loss 1.4426\n",
      "Epoch 1 Batch 179 Loss 1.9060\n",
      "Epoch 1 Batch 180 Loss 2.0961\n",
      "Epoch 1 Batch 181 Loss 1.5801\n",
      "Epoch 1 Batch 182 Loss 1.7943\n",
      "Epoch 1 Batch 183 Loss 1.6416\n",
      "Epoch 1 Batch 184 Loss 1.7324\n",
      "Epoch 1 Batch 185 Loss 1.7520\n",
      "Epoch 1 Batch 186 Loss 1.9026\n",
      "Epoch 1 Batch 187 Loss 1.5770\n",
      "Epoch 1 Batch 188 Loss 1.6089\n",
      "Epoch 1 Batch 189 Loss 1.7309\n",
      "Epoch 1 Batch 190 Loss 1.9407\n",
      "Epoch 1 Batch 191 Loss 1.5325\n",
      "Epoch 1 Batch 192 Loss 1.8072\n",
      "Epoch 1 Batch 193 Loss 1.9343\n",
      "Epoch 1 Batch 194 Loss 1.8834\n",
      "Epoch 1 Batch 195 Loss 1.7930\n",
      "Epoch 1 Batch 196 Loss 1.8384\n",
      "Epoch 1 Batch 197 Loss 1.6193\n",
      "Epoch 1 Batch 198 Loss 1.4773\n",
      "Epoch 1 Batch 199 Loss 1.7974\n",
      "Epoch 1 Batch 200 Loss 2.0404\n",
      "Epoch 1 Batch 201 Loss 1.7749\n",
      "Epoch 1 Batch 202 Loss 1.6444\n",
      "Epoch 1 Batch 203 Loss 1.7946\n",
      "Epoch 1 Batch 204 Loss 1.8486\n",
      "Epoch 1 Batch 205 Loss 1.6552\n",
      "Epoch 1 Batch 206 Loss 1.6512\n",
      "Epoch 1 Batch 207 Loss 1.9031\n",
      "Epoch 1 Batch 208 Loss 1.6629\n",
      "Epoch 1 Batch 209 Loss 1.6660\n",
      "Epoch 1 Batch 210 Loss 1.7076\n",
      "Epoch 1 Batch 211 Loss 1.9424\n",
      "Epoch 1 Batch 212 Loss 1.7210\n",
      "Epoch 1 Batch 213 Loss 1.7429\n",
      "Epoch 1 Batch 214 Loss 1.8415\n",
      "Epoch 1 Batch 215 Loss 1.5063\n",
      "Epoch 1 Batch 216 Loss 1.5858\n",
      "Epoch 1 Batch 217 Loss 1.7070\n",
      "Epoch 1 Batch 218 Loss 1.7336\n",
      "Epoch 1 Batch 219 Loss 1.6773\n",
      "Epoch 1 Batch 220 Loss 1.6055\n",
      "Epoch 1 Batch 221 Loss 1.6405\n",
      "Epoch 1 Batch 222 Loss 1.5156\n",
      "Epoch 1 Batch 223 Loss 1.8417\n",
      "Epoch 1 Batch 224 Loss 1.9008\n",
      "Epoch 1 Batch 225 Loss 1.6663\n",
      "Epoch 1 Batch 226 Loss 1.8153\n",
      "Epoch 1 Batch 227 Loss 1.7732\n",
      "Epoch 1 Batch 228 Loss 1.6923\n",
      "Epoch 1 Batch 229 Loss 1.5931\n",
      "Epoch 1 Batch 230 Loss 1.8837\n",
      "Epoch 1 Batch 231 Loss 1.9871\n",
      "Epoch 1 Batch 232 Loss 1.7856\n",
      "Epoch 1 Batch 233 Loss 1.5918\n",
      "Epoch 1 Batch 234 Loss 1.8009\n",
      "Epoch 1 Batch 235 Loss 1.8617\n",
      "Epoch 1 Batch 236 Loss 1.7448\n",
      "Epoch 1 Batch 237 Loss 1.7389\n",
      "Epoch 1 Batch 238 Loss 1.7329\n",
      "Epoch 1 Batch 239 Loss 1.6761\n",
      "Epoch 1 Batch 240 Loss 1.7679\n",
      "Epoch 1 Batch 241 Loss 1.6805\n",
      "Epoch 1 Batch 242 Loss 1.8289\n",
      "Epoch 1 Batch 243 Loss 1.8449\n",
      "Epoch 1 Batch 244 Loss 1.3586\n",
      "Epoch 1 Batch 245 Loss 1.6973\n",
      "Epoch 1 Batch 246 Loss 1.9131\n",
      "Epoch 1 Batch 247 Loss 1.6785\n",
      "Epoch 1 Batch 248 Loss 1.7286\n",
      "Epoch 1 Batch 249 Loss 1.8814\n",
      "Epoch 1 Batch 250 Loss 1.8318\n",
      "Epoch 1 Batch 251 Loss 1.4866\n",
      "Epoch 1 Batch 252 Loss 2.1582\n",
      "Epoch 1 Batch 253 Loss 1.5546\n",
      "Epoch 1 Batch 254 Loss 1.6154\n",
      "Epoch 1 Batch 255 Loss 2.0899\n",
      "Epoch 1 Batch 256 Loss 1.6160\n",
      "Epoch 1 Batch 257 Loss 1.6193\n",
      "Epoch 1 Batch 258 Loss 1.7116\n",
      "Epoch 1 Batch 259 Loss 1.5674\n",
      "Epoch 1 Batch 260 Loss 1.5595\n",
      "Epoch 1 Batch 261 Loss 1.7972\n",
      "Epoch 1 Batch 262 Loss 1.6695\n",
      "Epoch 1 Batch 263 Loss 1.8322\n",
      "Epoch 1 Batch 264 Loss 1.3737\n",
      "Epoch 1 Batch 265 Loss 1.5536\n",
      "Epoch 1 Batch 266 Loss 1.6744\n",
      "Epoch 1 Batch 267 Loss 1.6376\n",
      "Epoch 1 Batch 268 Loss 1.7638\n",
      "Epoch 1 Batch 269 Loss 1.7110\n",
      "Epoch 1 Batch 270 Loss 1.5885\n",
      "Epoch 1 Batch 271 Loss 1.7302\n",
      "Epoch 1 Batch 272 Loss 1.6109\n",
      "Epoch 1 Batch 273 Loss 1.9644\n",
      "Epoch 1 Batch 274 Loss 1.8845\n",
      "Epoch 1 Batch 275 Loss 1.7763\n",
      "Epoch 1 Batch 276 Loss 1.8157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 277 Loss 1.6354\n",
      "Epoch 1 Batch 278 Loss 1.7325\n",
      "Epoch 1 Batch 279 Loss 1.5126\n",
      "Epoch 1 Batch 280 Loss 1.6783\n",
      "Epoch 1 Batch 281 Loss 1.7230\n",
      "Epoch 1 Batch 282 Loss 1.4956\n",
      "Epoch 1 Batch 283 Loss 1.7511\n",
      "Epoch 1 Batch 284 Loss 1.8126\n",
      "Epoch 1 Batch 285 Loss 1.8181\n",
      "Epoch 1 Batch 286 Loss 1.5306\n",
      "Epoch 1 Batch 287 Loss 1.4782\n",
      "Epoch 1 Batch 288 Loss 1.6348\n",
      "Epoch 1 Batch 289 Loss 1.7804\n",
      "Epoch 1 Batch 290 Loss 1.6665\n",
      "Epoch 1 Batch 291 Loss 1.6000\n",
      "Epoch 1 Batch 292 Loss 1.8739\n",
      "Epoch 1 Batch 293 Loss 1.4917\n",
      "Epoch 1 Batch 294 Loss 1.7162\n",
      "Epoch 1 Batch 295 Loss 1.8978\n",
      "Epoch 1 Batch 296 Loss 1.6560\n",
      "Epoch 1 Batch 297 Loss 1.9214\n",
      "Epoch 1 Batch 298 Loss 1.4441\n",
      "Epoch 1 Batch 299 Loss 1.7124\n",
      "Epoch 1 Batch 300 Loss 1.8176\n",
      "Epoch 1 Batch 301 Loss 1.7370\n",
      "Epoch 1 Batch 302 Loss 1.5475\n",
      "Epoch 1 Batch 303 Loss 1.8976\n",
      "Epoch 1 Batch 304 Loss 1.8032\n",
      "Epoch 1 Batch 305 Loss 1.3739\n",
      "Epoch 1 Batch 306 Loss 1.5118\n",
      "Epoch 1 Batch 307 Loss 1.6639\n",
      "Epoch 1 Batch 308 Loss 1.8734\n",
      "Epoch 1 Batch 309 Loss 1.3786\n",
      "Epoch 1 Batch 310 Loss 1.5744\n",
      "Epoch 1 Batch 311 Loss 1.6657\n",
      "Epoch 1 Batch 312 Loss 1.7157\n",
      "Epoch 1 Batch 313 Loss 1.5809\n",
      "Epoch 1 Batch 314 Loss 1.6468\n",
      "Epoch 1 Batch 315 Loss 1.6949\n",
      "Epoch 1 Batch 316 Loss 1.5754\n",
      "Epoch 1 Batch 317 Loss 1.5581\n",
      "Epoch 1 Batch 318 Loss 1.6311\n",
      "Epoch 1 Batch 319 Loss 1.6800\n",
      "Epoch 1 Batch 320 Loss 1.5826\n",
      "Epoch 1 Batch 321 Loss 1.6169\n",
      "Epoch 1 Batch 322 Loss 1.7353\n",
      "Epoch 1 Batch 323 Loss 1.5562\n",
      "Epoch 1 Batch 324 Loss 1.5472\n",
      "Epoch 1 Batch 325 Loss 1.5747\n",
      "Epoch 1 Batch 326 Loss 1.7912\n",
      "Epoch 1 Batch 327 Loss 1.9321\n",
      "Epoch 1 Batch 328 Loss 1.5636\n",
      "Epoch 1 Batch 329 Loss 1.5870\n",
      "Epoch 1 Batch 330 Loss 1.4898\n",
      "Epoch 1 Batch 331 Loss 1.5127\n",
      "Epoch 1 Batch 332 Loss 1.6328\n",
      "Epoch 1 Batch 333 Loss 1.6435\n",
      "Epoch 1 Batch 334 Loss 1.6915\n",
      "Epoch 1 Batch 335 Loss 1.6999\n",
      "Epoch 1 Batch 336 Loss 1.6933\n",
      "Epoch 1 Batch 337 Loss 1.7164\n",
      "Epoch 1 Batch 338 Loss 1.8600\n",
      "Epoch 1 Batch 339 Loss 1.6377\n",
      "Epoch 1 Batch 340 Loss 1.5579\n",
      "Epoch 1 Batch 341 Loss 1.7794\n",
      "Epoch 1 Batch 342 Loss 1.8301\n",
      "Epoch 1 Batch 343 Loss 1.5332\n",
      "Epoch 1 Batch 344 Loss 1.8682\n",
      "Epoch 1 Batch 345 Loss 1.6115\n",
      "Epoch 1 Batch 346 Loss 1.7482\n",
      "Epoch 1 Batch 347 Loss 1.6569\n",
      "Epoch 1 Batch 348 Loss 1.5991\n",
      "Epoch 1 Batch 349 Loss 1.6889\n",
      "Epoch 1 Batch 350 Loss 1.4254\n",
      "Epoch 1 Batch 351 Loss 1.5275\n",
      "Epoch 1 Batch 352 Loss 1.6394\n",
      "Epoch 1 Batch 353 Loss 1.6264\n",
      "Epoch 1 Batch 354 Loss 1.5685\n",
      "Epoch 1 Batch 355 Loss 1.6526\n",
      "Epoch 1 Batch 356 Loss 1.4843\n",
      "Epoch 1 Batch 357 Loss 1.5553\n",
      "Epoch 1 Batch 358 Loss 1.6051\n",
      "Epoch 1 Batch 359 Loss 1.7667\n",
      "Epoch 1 Batch 360 Loss 1.5062\n",
      "Epoch 1 Batch 361 Loss 1.6118\n",
      "Epoch 1 Batch 362 Loss 1.5232\n",
      "Epoch 1 Batch 363 Loss 1.5817\n",
      "Epoch 1 Batch 364 Loss 1.6087\n",
      "Epoch 1 Batch 365 Loss 1.8929\n",
      "Epoch 1 Batch 366 Loss 1.4745\n",
      "Epoch 1 Batch 367 Loss 1.5873\n",
      "Epoch 1 Batch 368 Loss 1.5668\n",
      "Epoch 1 Batch 369 Loss 1.4970\n",
      "Epoch 1 Batch 370 Loss 1.4425\n",
      "Epoch 1 Batch 371 Loss 1.7876\n",
      "Epoch 1 Batch 372 Loss 1.7000\n",
      "Epoch 1 Batch 373 Loss 1.5607\n",
      "Epoch 1 Batch 374 Loss 1.7162\n",
      "Epoch 1 Batch 375 Loss 1.7487\n",
      "Epoch 1 Batch 376 Loss 1.6730\n",
      "Epoch 1 Batch 377 Loss 1.6277\n",
      "Epoch 1 Batch 378 Loss 1.6081\n",
      "Epoch 1 Batch 379 Loss 1.4985\n",
      "Epoch 1 Batch 380 Loss 1.6587\n",
      "Epoch 1 Batch 381 Loss 1.6590\n",
      "Epoch 1 Batch 382 Loss 1.3864\n",
      "Epoch 1 Batch 383 Loss 2.0119\n",
      "Epoch 1 Batch 384 Loss 1.6564\n",
      "Epoch 1 Batch 385 Loss 1.6218\n",
      "Epoch 1 Batch 386 Loss 1.7136\n",
      "Epoch 1 Batch 387 Loss 1.7917\n",
      "Epoch 1 Batch 388 Loss 1.7004\n",
      "Epoch 1 Batch 389 Loss 1.6266\n",
      "Epoch 1 Batch 390 Loss 1.7046\n",
      "Epoch 1 Batch 391 Loss 1.6449\n",
      "Epoch 1 Batch 392 Loss 1.8311\n",
      "Epoch 1 Batch 393 Loss 1.6144\n",
      "Epoch 1 Batch 394 Loss 1.8380\n",
      "Epoch 1 Batch 395 Loss 1.5863\n",
      "Epoch 1 Batch 396 Loss 1.7939\n",
      "Epoch 1 Batch 397 Loss 1.6171\n",
      "Epoch 1 Batch 398 Loss 1.7616\n",
      "Epoch 1 Batch 399 Loss 1.6227\n",
      "Epoch 1 Batch 400 Loss 1.8888\n",
      "Epoch 1 Batch 401 Loss 1.8973\n",
      "Epoch 1 Batch 402 Loss 1.5070\n",
      "Epoch 1 Batch 403 Loss 1.8299\n",
      "Epoch 1 Batch 404 Loss 1.8172\n",
      "Epoch 1 Batch 405 Loss 1.6651\n",
      "Epoch 1 Batch 406 Loss 1.6003\n",
      "Epoch 1 Batch 407 Loss 1.7796\n",
      "Epoch 1 Batch 408 Loss 1.8091\n",
      "Epoch 1 Batch 409 Loss 1.6665\n",
      "Epoch 1 Batch 410 Loss 1.6460\n",
      "Epoch 1 Batch 411 Loss 1.7076\n",
      "Epoch 1 Batch 412 Loss 1.7293\n",
      "Epoch 1 Batch 413 Loss 1.7556\n",
      "Epoch 1 Batch 414 Loss 1.6336\n",
      "Epoch 1 Batch 415 Loss 1.5808\n",
      "Epoch 1 Batch 416 Loss 1.6423\n",
      "Epoch 1 Batch 417 Loss 1.6176\n",
      "Epoch 1 Batch 418 Loss 1.6517\n",
      "Epoch 1 Batch 419 Loss 2.1388\n",
      "Epoch 1 Batch 420 Loss 1.4478\n",
      "Epoch 1 Batch 421 Loss 1.6264\n",
      "Epoch 1 Batch 422 Loss 1.6203\n",
      "Epoch 1 Batch 423 Loss 1.5028\n",
      "Epoch 1 Batch 424 Loss 1.5112\n",
      "Epoch 1 Batch 425 Loss 1.6257\n",
      "Epoch 1 Batch 426 Loss 1.6912\n",
      "Epoch 1 Batch 427 Loss 1.6600\n",
      "Epoch 1 Batch 428 Loss 1.6252\n",
      "Epoch 1 Batch 429 Loss 1.6351\n",
      "Epoch 1 Batch 430 Loss 1.5741\n",
      "Epoch 1 Batch 431 Loss 1.6007\n",
      "Epoch 1 Batch 432 Loss 1.4981\n",
      "Epoch 1 Batch 433 Loss 1.5717\n",
      "Epoch 1 Batch 434 Loss 1.6543\n",
      "Epoch 1 Batch 435 Loss 2.0986\n",
      "Epoch 1 Batch 436 Loss 1.6747\n",
      "Epoch 1 Batch 437 Loss 1.5159\n",
      "Epoch 1 Batch 438 Loss 1.6871\n",
      "Epoch 1 Batch 439 Loss 1.6930\n",
      "Epoch 1 Batch 440 Loss 1.6833\n",
      "Epoch 1 Batch 441 Loss 1.5002\n",
      "Epoch 1 Batch 442 Loss 1.7429\n",
      "Epoch 1 Batch 443 Loss 1.8787\n",
      "Epoch 1 Batch 444 Loss 1.5729\n",
      "Epoch 1 Batch 445 Loss 1.5539\n",
      "Epoch 1 Batch 446 Loss 1.5686\n",
      "Epoch 1 Batch 447 Loss 1.7413\n",
      "Epoch 1 Batch 448 Loss 1.5695\n",
      "Epoch 1 Batch 449 Loss 1.8847\n",
      "Epoch 1 Batch 450 Loss 1.5778\n",
      "Epoch 1 Batch 451 Loss 1.7501\n",
      "Epoch 1 Batch 452 Loss 1.7682\n",
      "Epoch 1 Batch 453 Loss 1.6530\n",
      "Epoch 1 Batch 454 Loss 1.5697\n",
      "Epoch 1 Batch 455 Loss 1.7015\n",
      "Epoch 1 Batch 456 Loss 1.7247\n",
      "Epoch 1 Batch 457 Loss 1.8371\n",
      "Epoch 1 Batch 458 Loss 1.3189\n",
      "Epoch 1 Batch 459 Loss 1.5829\n",
      "Epoch 1 Batch 460 Loss 1.3779\n",
      "Epoch 1 Batch 461 Loss 1.7423\n",
      "Epoch 1 Batch 462 Loss 1.4317\n",
      "Epoch 1 Batch 463 Loss 1.2789\n",
      "Epoch 1 Batch 464 Loss 1.7686\n",
      "Epoch 1 Batch 465 Loss 1.4307\n",
      "Epoch 1 Batch 466 Loss 1.4639\n",
      "Epoch 1 Batch 467 Loss 1.6437\n",
      "Epoch 1 Batch 468 Loss 1.4823\n",
      "Epoch 1 Batch 469 Loss 1.6831\n",
      "Epoch 1 Batch 470 Loss 1.4376\n",
      "Epoch 1 Batch 471 Loss 1.7305\n",
      "Epoch 1 Batch 472 Loss 1.6038\n",
      "Epoch 1 Batch 473 Loss 1.5878\n",
      "Epoch 1 Batch 474 Loss 1.6829\n",
      "Epoch 1 Batch 475 Loss 1.9823\n",
      "Epoch 1 Batch 476 Loss 1.6631\n",
      "Epoch 1 Batch 477 Loss 1.6432\n",
      "Epoch 1 Batch 478 Loss 1.8129\n",
      "Epoch 1 Batch 479 Loss 1.7289\n",
      "Epoch 1 Batch 480 Loss 1.4887\n",
      "Epoch 1 Batch 481 Loss 1.7050\n",
      "Epoch 1 Batch 482 Loss 1.7858\n",
      "Epoch 1 Batch 483 Loss 1.8720\n",
      "Epoch 1 Batch 484 Loss 1.7547\n",
      "Epoch 1 Batch 485 Loss 1.4968\n",
      "Epoch 1 Batch 486 Loss 1.3863\n",
      "Epoch 1 Batch 487 Loss 1.5569\n",
      "Epoch 1 Batch 488 Loss 1.6324\n",
      "Epoch 1 Batch 489 Loss 1.4796\n",
      "Epoch 1 Batch 490 Loss 1.8191\n",
      "Epoch 1 Batch 491 Loss 1.7676\n",
      "Epoch 1 Batch 492 Loss 1.6036\n",
      "Epoch 1 Batch 493 Loss 1.5403\n",
      "Epoch 1 Batch 494 Loss 1.7216\n",
      "Epoch 1 Batch 495 Loss 1.7962\n",
      "Epoch 1 Batch 496 Loss 1.6767\n",
      "Epoch 1 Batch 497 Loss 1.8507\n",
      "Epoch 1 Batch 498 Loss 1.5914\n",
      "Epoch 1 Batch 499 Loss 1.7981\n",
      "Epoch 1 Batch 500 Loss 1.7971\n",
      "Epoch 1 Batch 501 Loss 1.5662\n",
      "Epoch 1 Batch 502 Loss 1.5981\n",
      "Epoch 1 Batch 503 Loss 1.4936\n",
      "Epoch 1 Batch 504 Loss 1.5958\n",
      "Epoch 1 Batch 505 Loss 1.6540\n",
      "Epoch 1 Batch 506 Loss 1.6092\n",
      "Epoch 1 Batch 507 Loss 1.8064\n",
      "Epoch 1 Batch 508 Loss 1.6871\n",
      "Epoch 1 Batch 509 Loss 1.4314\n",
      "Epoch 1 Batch 510 Loss 1.4030\n",
      "Epoch 1 Batch 511 Loss 1.5721\n",
      "Epoch 1 Batch 512 Loss 1.5595\n",
      "Epoch 1 Batch 513 Loss 1.6204\n",
      "Epoch 1 Batch 514 Loss 1.6116\n",
      "Epoch 1 Batch 515 Loss 1.7782\n",
      "Epoch 1 Batch 516 Loss 1.6466\n",
      "Epoch 1 Batch 517 Loss 1.7689\n",
      "Epoch 1 Batch 518 Loss 1.6561\n",
      "Epoch 1 Batch 519 Loss 1.8004\n",
      "Epoch 1 Batch 520 Loss 1.8859\n",
      "Epoch 1 Batch 521 Loss 1.6968\n",
      "Epoch 1 Batch 522 Loss 1.5451\n",
      "Epoch 1 Batch 523 Loss 1.6194\n",
      "Epoch 1 Batch 524 Loss 1.7039\n",
      "Epoch 1 Batch 525 Loss 1.8605\n",
      "Epoch 1 Batch 526 Loss 1.6229\n",
      "Epoch 1 Batch 527 Loss 1.9148\n",
      "Epoch 1 Batch 528 Loss 1.4527\n",
      "Epoch 1 Batch 529 Loss 1.5518\n",
      "Epoch 1 Batch 530 Loss 1.6586\n",
      "Epoch 1 Batch 531 Loss 1.5401\n",
      "Epoch 1 Batch 532 Loss 1.5393\n",
      "Epoch 1 Batch 533 Loss 1.6895\n",
      "Epoch 1 Batch 534 Loss 1.5139\n",
      "Epoch 1 Batch 535 Loss 1.7947\n",
      "Epoch 1 Batch 536 Loss 1.6558\n",
      "Epoch 1 Batch 537 Loss 1.6346\n",
      "Epoch 1 Batch 538 Loss 1.6747\n",
      "Epoch 1 Batch 539 Loss 1.6562\n",
      "Epoch 1 Batch 540 Loss 1.5298\n",
      "Epoch 1 Batch 541 Loss 1.7837\n",
      "Epoch 1 Batch 542 Loss 1.7647\n",
      "Epoch 1 Batch 543 Loss 1.4340\n",
      "Epoch 1 Batch 544 Loss 1.6925\n",
      "Epoch 1 Batch 545 Loss 1.5797\n",
      "Epoch 1 Batch 546 Loss 1.7983\n",
      "Epoch 1 Batch 547 Loss 1.8138\n",
      "Epoch 1 Batch 548 Loss 1.6035\n",
      "Epoch 1 Batch 549 Loss 1.7909\n",
      "Epoch 1 Batch 550 Loss 1.6263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 551 Loss 1.7229\n",
      "Epoch 1 Batch 552 Loss 1.5354\n",
      "Epoch 1 Batch 553 Loss 1.7413\n",
      "Epoch 1 Batch 554 Loss 1.8816\n",
      "Epoch 1 Batch 555 Loss 1.4994\n",
      "Epoch 1 Batch 556 Loss 1.7784\n",
      "Epoch 1 Batch 557 Loss 1.6462\n",
      "Epoch 1 Batch 558 Loss 1.7875\n",
      "Epoch 1 Batch 559 Loss 1.5182\n",
      "Epoch 1 Batch 560 Loss 1.7288\n",
      "Epoch 1 Batch 561 Loss 1.7851\n",
      "Epoch 1 Batch 562 Loss 1.6156\n",
      "Epoch 1 Batch 563 Loss 1.7501\n",
      "Epoch 1 Batch 564 Loss 1.6909\n",
      "Epoch 1 Batch 565 Loss 1.5248\n",
      "Epoch 1 Batch 566 Loss 1.5547\n",
      "Epoch 1 Batch 567 Loss 1.5486\n",
      "Epoch 1 Batch 568 Loss 1.4508\n",
      "Epoch 1 Batch 569 Loss 1.5807\n",
      "Epoch 1 Batch 570 Loss 1.8115\n",
      "Epoch 1 Batch 571 Loss 1.8282\n",
      "Epoch 1 Batch 572 Loss 1.4594\n",
      "Epoch 1 Batch 573 Loss 1.4305\n",
      "Epoch 1 Batch 574 Loss 1.7038\n",
      "Epoch 1 Batch 575 Loss 1.8445\n",
      "Epoch 1 Batch 576 Loss 1.5242\n",
      "Epoch 1 Batch 577 Loss 1.6085\n",
      "Epoch 1 Batch 578 Loss 1.4914\n",
      "Epoch 1 Batch 579 Loss 1.6205\n",
      "Epoch 1 Batch 580 Loss 1.6294\n",
      "Epoch 1 Batch 581 Loss 1.6793\n",
      "Epoch 1 Batch 582 Loss 1.4649\n",
      "Epoch 1 Batch 583 Loss 1.5062\n",
      "Epoch 1 Batch 584 Loss 1.9060\n",
      "Epoch 1 Batch 585 Loss 1.8258\n",
      "Epoch 1 Batch 586 Loss 1.5412\n",
      "Epoch 1 Batch 587 Loss 1.6088\n",
      "Epoch 1 Batch 588 Loss 1.5809\n",
      "Epoch 1 Batch 589 Loss 1.7962\n",
      "Epoch 1 Batch 590 Loss 1.7281\n",
      "Epoch 1 Batch 591 Loss 1.4092\n",
      "Epoch 1 Batch 592 Loss 1.5389\n",
      "Epoch 1 Batch 593 Loss 1.4389\n",
      "Epoch 1 Batch 594 Loss 1.7171\n",
      "Epoch 1 Batch 595 Loss 1.6620\n",
      "Epoch 1 Batch 596 Loss 1.7571\n",
      "Epoch 1 Batch 597 Loss 1.7352\n",
      "Epoch 1 Batch 598 Loss 1.5092\n",
      "Epoch 1 Batch 599 Loss 1.3460\n",
      "Epoch 1 Batch 600 Loss 1.6799\n",
      "Epoch 1 Batch 601 Loss 1.6526\n",
      "Epoch 1 Batch 602 Loss 1.6665\n",
      "Epoch 1 Batch 603 Loss 1.5626\n",
      "Epoch 1 Batch 604 Loss 1.5338\n",
      "Epoch 1 Batch 605 Loss 1.6089\n",
      "Epoch 1 Batch 606 Loss 1.8107\n",
      "Epoch 1 Batch 607 Loss 1.7718\n",
      "Epoch 1 Batch 608 Loss 1.4587\n",
      "Epoch 1 Batch 609 Loss 1.5228\n",
      "Epoch 1 Batch 610 Loss 1.7867\n",
      "Epoch 1 Batch 611 Loss 1.6582\n",
      "Epoch 1 Batch 612 Loss 1.8166\n",
      "Epoch 1 Batch 613 Loss 1.5465\n",
      "Epoch 1 Batch 614 Loss 1.6557\n",
      "Epoch 1 Batch 615 Loss 1.6533\n",
      "Epoch 1 Batch 616 Loss 1.7669\n",
      "Epoch 1 Batch 617 Loss 1.4777\n",
      "Epoch 1 Batch 618 Loss 1.4366\n",
      "Epoch 1 Batch 619 Loss 1.8383\n",
      "Epoch 1 Batch 620 Loss 1.5265\n",
      "Epoch 1 Batch 621 Loss 1.8063\n",
      "Epoch 1 Batch 622 Loss 1.7146\n",
      "Epoch 1 Batch 623 Loss 1.6497\n",
      "Epoch 1 Batch 624 Loss 1.7639\n",
      "Epoch 1 Batch 625 Loss 1.4470\n",
      "Epoch 1 Batch 626 Loss 1.4021\n",
      "Epoch 1 Batch 627 Loss 1.6057\n",
      "Epoch 1 Batch 628 Loss 1.7118\n",
      "Epoch 1 Batch 629 Loss 1.6283\n",
      "Epoch 1 Batch 630 Loss 1.4133\n",
      "Epoch 1 Batch 631 Loss 1.6752\n",
      "Epoch 1 Batch 632 Loss 1.6667\n",
      "Epoch 1 Batch 633 Loss 1.5413\n",
      "Epoch 1 Batch 634 Loss 1.4911\n",
      "Epoch 1 Batch 635 Loss 1.5016\n",
      "Epoch 1 Batch 636 Loss 1.5796\n",
      "Epoch 1 Batch 637 Loss 1.5359\n",
      "Epoch 1 Batch 638 Loss 1.6930\n",
      "Epoch 1 Batch 639 Loss 1.5620\n",
      "Epoch 1 Batch 640 Loss 1.5721\n",
      "Epoch 1 Batch 641 Loss 1.5931\n",
      "Epoch 1 Batch 642 Loss 1.5549\n",
      "Epoch 1 Batch 643 Loss 1.8515\n",
      "Epoch 1 Batch 644 Loss 1.5097\n",
      "Epoch 1 Batch 645 Loss 1.8131\n",
      "Epoch 1 Batch 646 Loss 1.3900\n",
      "Epoch 1 Batch 647 Loss 1.7568\n",
      "Epoch 1 Batch 648 Loss 1.5815\n",
      "Epoch 1 Batch 649 Loss 1.5424\n",
      "Epoch 1 Batch 650 Loss 1.7026\n",
      "Epoch 1 Batch 651 Loss 1.5329\n",
      "Epoch 1 Batch 652 Loss 1.5956\n",
      "Epoch 1 Batch 653 Loss 1.6056\n",
      "Epoch 1 Batch 654 Loss 1.4762\n",
      "Epoch 1 Batch 655 Loss 1.6416\n",
      "Epoch 1 Batch 656 Loss 1.8245\n",
      "Epoch 1 Batch 657 Loss 1.6218\n",
      "Epoch 1 Batch 658 Loss 1.6686\n",
      "Epoch 1 Batch 659 Loss 1.6935\n",
      "Epoch 1 Batch 660 Loss 1.7228\n",
      "Epoch 1 Batch 661 Loss 1.7671\n",
      "Epoch 1 Batch 662 Loss 1.8087\n",
      "Epoch 1 Batch 663 Loss 1.3957\n",
      "Epoch 1 Batch 664 Loss 1.4677\n",
      "Epoch 1 Batch 665 Loss 1.5943\n",
      "Epoch 1 Batch 666 Loss 1.7652\n",
      "Epoch 1 Batch 667 Loss 1.6234\n",
      "Epoch 1 Batch 668 Loss 1.6454\n",
      "Epoch 1 Batch 669 Loss 1.6750\n",
      "Epoch 1 Batch 670 Loss 1.8288\n",
      "Epoch 1 Batch 671 Loss 1.5973\n",
      "Epoch 1 Batch 672 Loss 1.3359\n",
      "Epoch 1 Batch 673 Loss 1.5750\n",
      "Epoch 1 Batch 674 Loss 1.5154\n",
      "Epoch 1 Batch 675 Loss 1.4921\n",
      "Epoch 1 Batch 676 Loss 1.6478\n",
      "Epoch 1 Batch 677 Loss 1.6046\n",
      "Epoch 1 Batch 678 Loss 1.4161\n",
      "Epoch 1 Batch 679 Loss 1.7537\n",
      "Epoch 1 Batch 680 Loss 1.6192\n",
      "Epoch 1 Batch 681 Loss 1.6047\n",
      "Epoch 1 Batch 682 Loss 1.8454\n",
      "Epoch 1 Batch 683 Loss 1.5977\n",
      "Epoch 1 Batch 684 Loss 1.6550\n",
      "Epoch 1 Batch 685 Loss 1.5562\n",
      "Epoch 1 Batch 686 Loss 1.5047\n",
      "Epoch 1 Batch 687 Loss 1.5826\n",
      "Epoch 1 Batch 688 Loss 1.4696\n",
      "Epoch 1 Batch 689 Loss 1.6433\n",
      "Epoch 1 Batch 690 Loss 1.4445\n",
      "Epoch 1 Batch 691 Loss 1.4380\n",
      "Epoch 1 Batch 692 Loss 1.5362\n",
      "Epoch 1 Batch 693 Loss 1.5740\n",
      "Epoch 1 Batch 694 Loss 1.6601\n",
      "Epoch 1 Batch 695 Loss 1.4721\n",
      "Epoch 1 Batch 696 Loss 1.6442\n",
      "Epoch 1 Batch 697 Loss 1.7110\n",
      "Epoch 1 Batch 698 Loss 1.6266\n",
      "Epoch 1 Batch 699 Loss 1.5704\n",
      "Epoch 1 Batch 700 Loss 1.8118\n",
      "Epoch 1 Batch 701 Loss 1.6978\n",
      "Epoch 1 Batch 702 Loss 1.4940\n",
      "Epoch 1 Batch 703 Loss 1.5972\n",
      "Epoch 1 Batch 704 Loss 1.7517\n",
      "Epoch 1 Batch 705 Loss 1.4936\n",
      "Epoch 1 Batch 706 Loss 1.4056\n",
      "Epoch 1 Batch 707 Loss 1.5699\n",
      "Epoch 1 Batch 708 Loss 1.8458\n",
      "Epoch 1 Batch 709 Loss 1.6997\n",
      "Epoch 1 Batch 710 Loss 1.5085\n",
      "Epoch 1 Batch 711 Loss 1.6455\n",
      "Epoch 1 Batch 712 Loss 1.7064\n",
      "Epoch 1 Batch 713 Loss 1.6175\n",
      "Epoch 1 Batch 714 Loss 1.5189\n",
      "Epoch 1 Batch 715 Loss 1.5333\n",
      "Epoch 1 Batch 716 Loss 1.7146\n",
      "Epoch 1 Batch 717 Loss 1.5192\n",
      "Epoch 1 Batch 718 Loss 1.4183\n",
      "Epoch 1 Batch 719 Loss 1.4667\n",
      "Epoch 1 Batch 720 Loss 1.5573\n",
      "Epoch 1 Batch 721 Loss 1.4646\n",
      "Epoch 1 Batch 722 Loss 1.8241\n",
      "Epoch 1 Batch 723 Loss 1.4989\n",
      "Epoch 1 Batch 724 Loss 1.6557\n",
      "Epoch 1 Batch 725 Loss 1.4755\n",
      "Epoch 1 Batch 726 Loss 1.3641\n",
      "Epoch 1 Batch 727 Loss 1.3808\n",
      "Epoch 1 Batch 728 Loss 1.7674\n",
      "Epoch 1 Batch 729 Loss 1.5144\n",
      "Epoch 1 Batch 730 Loss 1.5050\n",
      "Epoch 1 Batch 731 Loss 1.7651\n",
      "Epoch 1 Batch 732 Loss 1.5553\n",
      "Epoch 1 Batch 733 Loss 1.7057\n",
      "Epoch 1 Batch 734 Loss 1.3036\n",
      "Epoch 1 Batch 735 Loss 1.6575\n",
      "Epoch 1 Batch 736 Loss 1.5692\n",
      "Epoch 1 Batch 737 Loss 1.5860\n",
      "Epoch 1 Batch 738 Loss 1.4360\n",
      "Epoch 1 Batch 739 Loss 1.6901\n",
      "Epoch 1 Batch 740 Loss 1.6254\n",
      "Epoch 1 Batch 741 Loss 1.4488\n",
      "Epoch 1 Batch 742 Loss 1.7302\n",
      "Epoch 1 Batch 743 Loss 1.7463\n",
      "Epoch 1 Batch 744 Loss 1.5944\n",
      "Epoch 1 Batch 745 Loss 1.4606\n",
      "Epoch 1 Batch 746 Loss 1.4270\n",
      "Epoch 1 Batch 747 Loss 1.9737\n",
      "Epoch 1 Batch 748 Loss 1.6474\n",
      "Epoch 1 Batch 749 Loss 1.5188\n",
      "Epoch 1 Batch 750 Loss 1.3877\n",
      "Epoch 1 Batch 751 Loss 1.7255\n",
      "Epoch 1 Batch 752 Loss 1.6486\n",
      "Epoch 1 Batch 753 Loss 1.6834\n",
      "Epoch 1 Batch 754 Loss 1.6798\n",
      "Epoch 1 Batch 755 Loss 1.7658\n",
      "Epoch 1 Batch 756 Loss 1.4316\n",
      "Epoch 1 Batch 757 Loss 1.5418\n",
      "Epoch 1 Batch 758 Loss 1.6356\n",
      "Epoch 1 Batch 759 Loss 1.4534\n",
      "Epoch 1 Batch 760 Loss 1.7065\n",
      "Epoch 1 Batch 761 Loss 1.5059\n",
      "Epoch 1 Batch 762 Loss 1.5038\n",
      "Epoch 1 Batch 763 Loss 1.8114\n",
      "Epoch 1 Batch 764 Loss 1.6952\n",
      "Epoch 1 Batch 765 Loss 1.5246\n",
      "Epoch 1 Batch 766 Loss 1.5739\n",
      "Epoch 1 Batch 767 Loss 1.3149\n",
      "Epoch 1 Batch 768 Loss 1.3826\n",
      "Epoch 1 Batch 769 Loss 1.6006\n",
      "Epoch 1 Batch 770 Loss 1.3967\n",
      "Epoch 1 Batch 771 Loss 1.4559\n",
      "Epoch 1 Batch 772 Loss 1.5850\n",
      "Epoch 1 Batch 773 Loss 1.4680\n",
      "Epoch 1 Batch 774 Loss 1.5398\n",
      "Epoch 1 Batch 775 Loss 1.6204\n",
      "Epoch 1 Batch 776 Loss 1.6553\n",
      "Epoch 1 Batch 777 Loss 1.6435\n",
      "Epoch 1 Batch 778 Loss 1.4124\n",
      "Epoch 1 Batch 779 Loss 1.5342\n",
      "Epoch 1 Batch 780 Loss 1.4818\n",
      "Epoch 1 Batch 781 Loss 1.5531\n",
      "Epoch 1 Batch 782 Loss 1.4810\n",
      "Epoch 1 Batch 783 Loss 1.7654\n",
      "Epoch 1 Batch 784 Loss 1.6219\n",
      "Epoch 1 Batch 785 Loss 1.7329\n",
      "Epoch 1 Batch 786 Loss 1.8874\n",
      "Epoch 1 Batch 787 Loss 1.4600\n",
      "Epoch 1 Batch 788 Loss 1.6755\n",
      "Epoch 1 Batch 789 Loss 1.6291\n",
      "Epoch 1 Batch 790 Loss 1.6208\n",
      "Epoch 1 Batch 791 Loss 1.7776\n",
      "Epoch 1 Batch 792 Loss 1.4491\n",
      "Epoch 1 Batch 793 Loss 1.6725\n",
      "Epoch 1 Batch 794 Loss 1.8483\n",
      "Epoch 1 Batch 795 Loss 1.7656\n",
      "Epoch 1 Batch 796 Loss 1.7307\n",
      "Epoch 1 Batch 797 Loss 1.6105\n",
      "Epoch 1 Batch 798 Loss 1.8024\n",
      "Epoch 1 Batch 799 Loss 1.5783\n",
      "Epoch 1 Batch 800 Loss 1.6419\n",
      "Epoch 1 Batch 801 Loss 1.5537\n",
      "Epoch 1 Batch 802 Loss 1.6756\n",
      "Epoch 1 Batch 803 Loss 1.3526\n",
      "Epoch 1 Batch 804 Loss 1.5701\n",
      "Epoch 1 Batch 805 Loss 1.6439\n",
      "Epoch 1 Batch 806 Loss 1.1754\n",
      "Epoch 1 Batch 807 Loss 1.5562\n",
      "Epoch 1 Batch 808 Loss 1.7434\n",
      "Epoch 1 Batch 809 Loss 1.5870\n",
      "Epoch 1 Batch 810 Loss 1.5730\n",
      "Epoch 1 Batch 811 Loss 1.6214\n",
      "Epoch 1 Batch 812 Loss 1.7174\n",
      "Epoch 1 Batch 813 Loss 1.4563\n",
      "Epoch 1 Batch 814 Loss 1.6234\n",
      "Epoch 1 Batch 815 Loss 1.4324\n",
      "Epoch 1 Batch 816 Loss 1.5594\n",
      "Epoch 1 Batch 817 Loss 1.4984\n",
      "Epoch 1 Batch 818 Loss 1.4656\n",
      "Epoch 1 Batch 819 Loss 1.4763\n",
      "Epoch 1 Batch 820 Loss 1.5166\n",
      "Epoch 1 Batch 821 Loss 1.6563\n",
      "Epoch 1 Batch 822 Loss 1.7584\n",
      "Epoch 1 Batch 823 Loss 1.4245\n",
      "Epoch 1 Batch 824 Loss 1.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 825 Loss 1.5473\n",
      "Epoch 1 Batch 826 Loss 1.3929\n",
      "Epoch 1 Batch 827 Loss 1.6446\n",
      "Epoch 1 Batch 828 Loss 1.4980\n",
      "Epoch 1 Batch 829 Loss 1.7888\n",
      "Epoch 1 Batch 830 Loss 1.3807\n",
      "Epoch 1 Batch 831 Loss 1.5275\n",
      "Epoch 1 Batch 832 Loss 1.4965\n",
      "Epoch 1 Batch 833 Loss 1.6242\n",
      "Epoch 1 Batch 834 Loss 1.7284\n",
      "Epoch 1 Batch 835 Loss 1.6957\n",
      "Epoch 1 Batch 836 Loss 1.6109\n",
      "Epoch 1 Batch 837 Loss 1.7327\n",
      "Epoch 1 Batch 838 Loss 1.4282\n",
      "Epoch 1 Batch 839 Loss 1.5998\n",
      "Epoch 1 Batch 840 Loss 1.4262\n",
      "Epoch 1 Batch 841 Loss 1.7414\n",
      "Epoch 1 Batch 842 Loss 1.7600\n",
      "Epoch 1 Batch 843 Loss 1.4267\n",
      "Epoch 1 Batch 844 Loss 1.5812\n",
      "Epoch 1 Batch 845 Loss 1.3916\n",
      "Epoch 1 Batch 846 Loss 1.5777\n",
      "Epoch 1 Batch 847 Loss 1.6126\n",
      "Epoch 1 Batch 848 Loss 1.6760\n",
      "Epoch 1 Batch 849 Loss 1.4867\n",
      "Epoch 1 Batch 850 Loss 1.4694\n",
      "Epoch 1 Batch 851 Loss 1.6671\n",
      "Epoch 1 Batch 852 Loss 1.5274\n",
      "Epoch 1 Batch 853 Loss 1.4619\n",
      "Epoch 1 Batch 854 Loss 1.6093\n",
      "Epoch 1 Batch 855 Loss 1.4324\n",
      "Epoch 1 Batch 856 Loss 1.5157\n",
      "Epoch 1 Batch 857 Loss 1.5494\n",
      "Epoch 1 Batch 858 Loss 1.7116\n",
      "Epoch 1 Batch 859 Loss 1.6131\n",
      "Epoch 1 Batch 860 Loss 1.4531\n",
      "Epoch 1 Batch 861 Loss 1.6919\n",
      "Epoch 1 Batch 862 Loss 1.9438\n",
      "Epoch 1 Batch 863 Loss 1.5332\n",
      "Epoch 1 Batch 864 Loss 1.3605\n",
      "Epoch 1 Batch 865 Loss 1.5783\n",
      "Epoch 1 Batch 866 Loss 1.5537\n",
      "Epoch 1 Batch 867 Loss 1.5873\n",
      "Epoch 1 Batch 868 Loss 1.6442\n",
      "Epoch 1 Batch 869 Loss 1.4605\n",
      "Epoch 1 Batch 870 Loss 1.6359\n",
      "Epoch 1 Batch 871 Loss 1.5566\n",
      "Epoch 1 Batch 872 Loss 1.7363\n",
      "Epoch 1 Batch 873 Loss 1.7014\n",
      "Epoch 1 Batch 874 Loss 1.6251\n",
      "Epoch 1 Batch 875 Loss 1.7001\n",
      "Epoch 1 Batch 876 Loss 1.5140\n",
      "Epoch 1 Batch 877 Loss 1.5291\n",
      "Epoch 1 Batch 878 Loss 1.7257\n",
      "Epoch 1 Batch 879 Loss 1.5709\n",
      "Epoch 1 Batch 880 Loss 1.6682\n",
      "Epoch 1 Batch 881 Loss 1.9014\n",
      "Epoch 1 Batch 882 Loss 1.6347\n",
      "Epoch 1 Batch 883 Loss 1.6886\n",
      "Epoch 1 Batch 884 Loss 1.5075\n",
      "Epoch 1 Batch 885 Loss 1.6634\n",
      "Epoch 1 Batch 886 Loss 1.6384\n",
      "Epoch 1 Batch 887 Loss 1.4474\n",
      "Epoch 1 Batch 888 Loss 1.5893\n",
      "Epoch 1 Batch 889 Loss 1.4694\n",
      "Epoch 1 Batch 890 Loss 1.4694\n",
      "Epoch 1 Batch 891 Loss 1.5740\n",
      "Epoch 1 Batch 892 Loss 1.5751\n",
      "Epoch 1 Batch 893 Loss 1.6131\n",
      "Epoch 1 Batch 894 Loss 1.5273\n",
      "Epoch 1 Batch 895 Loss 1.7386\n",
      "Epoch 1 Batch 896 Loss 2.0012\n",
      "Epoch 1 Batch 897 Loss 1.4681\n",
      "Epoch 1 Batch 898 Loss 1.4469\n",
      "Epoch 1 Batch 899 Loss 1.5251\n",
      "Epoch 1 Batch 900 Loss 1.5932\n",
      "Epoch 1 Batch 901 Loss 1.6571\n",
      "Epoch 1 Batch 902 Loss 1.3284\n",
      "Epoch 1 Batch 903 Loss 1.6374\n",
      "Epoch 1 Batch 904 Loss 1.4840\n",
      "Epoch 1 Batch 905 Loss 1.8892\n",
      "Epoch 1 Batch 906 Loss 1.5581\n",
      "Epoch 1 Batch 907 Loss 1.2881\n",
      "Epoch 1 Batch 908 Loss 1.3593\n",
      "Epoch 1 Batch 909 Loss 1.4776\n",
      "Epoch 1 Batch 910 Loss 1.6701\n",
      "Epoch 1 Batch 911 Loss 1.3762\n",
      "Epoch 1 Batch 912 Loss 1.3538\n",
      "Epoch 1 Batch 913 Loss 1.5121\n",
      "Epoch 1 Batch 914 Loss 1.6070\n",
      "Epoch 1 Batch 915 Loss 1.4831\n",
      "Epoch 1 Batch 916 Loss 1.5087\n",
      "Epoch 1 Batch 917 Loss 1.5968\n",
      "Epoch 1 Batch 918 Loss 1.3724\n",
      "Epoch 1 Batch 919 Loss 1.5926\n",
      "Epoch 1 Batch 920 Loss 1.6682\n",
      "Epoch 1 Batch 921 Loss 1.7087\n",
      "Epoch 1 Batch 922 Loss 1.6680\n",
      "Epoch 1 Batch 923 Loss 1.6848\n",
      "Epoch 1 Batch 924 Loss 1.4304\n",
      "Epoch 1 Batch 925 Loss 1.6738\n",
      "Epoch 1 Batch 926 Loss 1.4995\n",
      "Epoch 1 Batch 927 Loss 1.5653\n",
      "Epoch 1 Batch 928 Loss 1.5054\n",
      "Epoch 1 Batch 929 Loss 1.5592\n",
      "Epoch 1 Batch 930 Loss 1.4025\n",
      "Epoch 1 Batch 931 Loss 1.4670\n",
      "Epoch 1 Batch 932 Loss 1.5778\n",
      "Epoch 1 Batch 933 Loss 1.7650\n",
      "Epoch 1 Batch 934 Loss 1.5494\n",
      "Epoch 1 Batch 935 Loss 1.3983\n",
      "Epoch 1 Batch 936 Loss 1.6531\n",
      "Epoch 1 Batch 937 Loss 1.8403\n",
      "Epoch 1 Batch 938 Loss 1.4110\n",
      "Epoch 1 Batch 939 Loss 1.5850\n",
      "Epoch 1 Batch 940 Loss 1.6227\n",
      "Epoch 1 Batch 941 Loss 1.5860\n",
      "Epoch 1 Batch 942 Loss 1.6770\n",
      "Epoch 1 Batch 943 Loss 1.4493\n",
      "Epoch 1 Batch 944 Loss 1.4946\n",
      "Epoch 1 Batch 945 Loss 1.4635\n",
      "Epoch 1 Batch 946 Loss 1.8160\n",
      "Epoch 1 Batch 947 Loss 1.4768\n",
      "Epoch 1 Batch 948 Loss 1.6177\n",
      "Epoch 1 Batch 949 Loss 1.5270\n",
      "Epoch 1 Batch 950 Loss 1.5615\n",
      "Epoch 1 Batch 951 Loss 1.7170\n",
      "Epoch 1 Batch 952 Loss 1.5561\n",
      "Epoch 1 Batch 953 Loss 1.5868\n",
      "Epoch 1 Batch 954 Loss 1.6628\n",
      "Epoch 1 Batch 955 Loss 1.5631\n",
      "Epoch 1 Batch 956 Loss 1.6065\n",
      "Epoch 1 Batch 957 Loss 1.4108\n",
      "Epoch 1 Batch 958 Loss 2.0478\n",
      "Epoch 1 Batch 959 Loss 1.4965\n",
      "Epoch 1 Batch 960 Loss 1.5963\n",
      "Epoch 1 Batch 961 Loss 1.6486\n",
      "Epoch 1 Batch 962 Loss 1.6291\n",
      "Epoch 1 Batch 963 Loss 1.5188\n",
      "Epoch 1 Batch 964 Loss 1.4629\n",
      "Epoch 1 Batch 965 Loss 1.9956\n",
      "Epoch 1 Batch 966 Loss 1.6624\n",
      "Epoch 1 Batch 967 Loss 1.5041\n",
      "Epoch 1 Batch 968 Loss 1.7403\n",
      "Epoch 1 Batch 969 Loss 1.3422\n",
      "Epoch 1 Batch 970 Loss 1.7026\n",
      "Epoch 1 Batch 971 Loss 1.3310\n",
      "Epoch 1 Batch 972 Loss 1.4536\n",
      "Epoch 1 Batch 973 Loss 1.6747\n",
      "Epoch 1 Batch 974 Loss 1.7461\n",
      "Epoch 1 Batch 975 Loss 1.3160\n",
      "Epoch 1 Batch 976 Loss 1.6617\n",
      "Epoch 1 Batch 977 Loss 1.3525\n",
      "Epoch 1 Batch 978 Loss 1.5256\n",
      "Epoch 1 Batch 979 Loss 1.7137\n",
      "Epoch 1 Batch 980 Loss 1.3828\n",
      "Epoch 1 Batch 981 Loss 1.6722\n",
      "Epoch 1 Batch 982 Loss 1.6804\n",
      "Epoch 1 Batch 983 Loss 1.6542\n",
      "Epoch 1 Batch 984 Loss 1.5765\n",
      "Epoch 1 Batch 985 Loss 1.6756\n",
      "Epoch 1 Batch 986 Loss 1.4413\n",
      "Epoch 1 Batch 987 Loss 1.5701\n",
      "Epoch 1 Batch 988 Loss 1.4694\n",
      "Epoch 1 Batch 989 Loss 1.5424\n",
      "Epoch 1 Batch 990 Loss 1.7476\n",
      "Epoch 1 Batch 991 Loss 1.4292\n",
      "Epoch 1 Batch 992 Loss 1.6230\n",
      "Epoch 1 Batch 993 Loss 1.6569\n",
      "Epoch 1 Batch 994 Loss 1.3650\n",
      "Epoch 1 Batch 995 Loss 1.3708\n",
      "Epoch 1 Batch 996 Loss 1.3736\n",
      "Epoch 1 Batch 997 Loss 1.3887\n",
      "Epoch 1 Batch 998 Loss 1.4255\n",
      "Epoch 1 Batch 999 Loss 1.7808\n",
      "Epoch 1 Batch 1000 Loss 1.5626\n",
      "Epoch 1 Batch 1001 Loss 1.6544\n",
      "Epoch 1 Batch 1002 Loss 1.5766\n",
      "Epoch 1 Batch 1003 Loss 1.6180\n",
      "Epoch 1 Batch 1004 Loss 1.6839\n",
      "Epoch 1 Batch 1005 Loss 1.8371\n",
      "Epoch 1 Batch 1006 Loss 1.7008\n",
      "Epoch 1 Batch 1007 Loss 1.5869\n",
      "Epoch 1 Batch 1008 Loss 1.4160\n",
      "Epoch 1 Batch 1009 Loss 1.5524\n",
      "Epoch 1 Batch 1010 Loss 1.5441\n",
      "Epoch 1 Batch 1011 Loss 1.5285\n",
      "Epoch 1 Batch 1012 Loss 1.4236\n",
      "Epoch 1 Batch 1013 Loss 1.3704\n",
      "Epoch 1 Batch 1014 Loss 1.6737\n",
      "Epoch 1 Batch 1015 Loss 1.6228\n",
      "Epoch 1 Batch 1016 Loss 1.7869\n",
      "Epoch 1 Batch 1017 Loss 1.4462\n",
      "Epoch 1 Batch 1018 Loss 1.4459\n",
      "Epoch 1 Batch 1019 Loss 1.6244\n",
      "Epoch 1 Batch 1020 Loss 1.5284\n",
      "Epoch 1 Batch 1021 Loss 1.6149\n",
      "Epoch 1 Batch 1022 Loss 1.4263\n",
      "Epoch 1 Batch 1023 Loss 1.6732\n",
      "Epoch 1 Batch 1024 Loss 1.6222\n",
      "Epoch 1 Batch 1025 Loss 1.6195\n",
      "Epoch 1 Batch 1026 Loss 1.5460\n",
      "Epoch 1 Batch 1027 Loss 1.3683\n",
      "Epoch 1 Batch 1028 Loss 1.5654\n",
      "Epoch 1 Batch 1029 Loss 1.5381\n",
      "Epoch 1 Batch 1030 Loss 1.3366\n",
      "Epoch 1 Batch 1031 Loss 1.5049\n",
      "Epoch 1 Batch 1032 Loss 1.5082\n",
      "Epoch 1 Batch 1033 Loss 1.6033\n",
      "Epoch 1 Batch 1034 Loss 1.5154\n",
      "Epoch 1 Batch 1035 Loss 1.5745\n",
      "Epoch 1 Batch 1036 Loss 1.3927\n",
      "Epoch 1 Batch 1037 Loss 1.4582\n",
      "Epoch 1 Batch 1038 Loss 1.4628\n",
      "Epoch 1 Batch 1039 Loss 1.5362\n",
      "Epoch 1 Batch 1040 Loss 1.6275\n",
      "Epoch 1 Batch 1041 Loss 1.4482\n",
      "Epoch 1 Batch 1042 Loss 1.4682\n",
      "Epoch 1 Batch 1043 Loss 1.5457\n",
      "Epoch 1 Batch 1044 Loss 1.4355\n",
      "Epoch 1 Batch 1045 Loss 1.3816\n",
      "Epoch 1 Batch 1046 Loss 1.6727\n",
      "Epoch 1 Batch 1047 Loss 1.4232\n",
      "Epoch 1 Batch 1048 Loss 1.5309\n",
      "Epoch 1 Batch 1049 Loss 1.6538\n",
      "Epoch 1 Batch 1050 Loss 1.5463\n",
      "Epoch 1 Batch 1051 Loss 1.7664\n",
      "Epoch 1 Batch 1052 Loss 1.5551\n",
      "Epoch 1 Batch 1053 Loss 1.4816\n",
      "Epoch 1 Batch 1054 Loss 1.4694\n",
      "Epoch 1 Batch 1055 Loss 1.5519\n",
      "Epoch 1 Batch 1056 Loss 1.6046\n",
      "Epoch 1 Batch 1057 Loss 1.7182\n",
      "Epoch 1 Batch 1058 Loss 1.3107\n",
      "Epoch 1 Batch 1059 Loss 1.5575\n",
      "Epoch 1 Batch 1060 Loss 1.4236\n",
      "Epoch 1 Batch 1061 Loss 1.6042\n",
      "Epoch 1 Batch 1062 Loss 1.4512\n",
      "Epoch 1 Batch 1063 Loss 1.4460\n",
      "Epoch 1 Batch 1064 Loss 1.3350\n",
      "Epoch 1 Batch 1065 Loss 1.5678\n",
      "Epoch 1 Batch 1066 Loss 1.7414\n",
      "Epoch 1 Batch 1067 Loss 1.3743\n",
      "Epoch 1 Batch 1068 Loss 1.6088\n",
      "Epoch 1 Batch 1069 Loss 1.4059\n",
      "Epoch 1 Batch 1070 Loss 1.6623\n",
      "Epoch 1 Batch 1071 Loss 1.6147\n",
      "Epoch 1 Batch 1072 Loss 1.4591\n",
      "Epoch 1 Batch 1073 Loss 1.2938\n",
      "Epoch 1 Batch 1074 Loss 1.5021\n",
      "Epoch 1 Batch 1075 Loss 1.4849\n",
      "Epoch 1 Batch 1076 Loss 1.3301\n",
      "Epoch 1 Batch 1077 Loss 1.6153\n",
      "Epoch 1 Batch 1078 Loss 1.4922\n",
      "Epoch 1 Batch 1079 Loss 1.7143\n",
      "Epoch 1 Batch 1080 Loss 1.4270\n",
      "Epoch 1 Batch 1081 Loss 1.5981\n",
      "Epoch 1 Batch 1082 Loss 1.8063\n",
      "Epoch 1 Batch 1083 Loss 1.7031\n",
      "Epoch 1 Batch 1084 Loss 1.7057\n",
      "Epoch 1 Batch 1085 Loss 1.3524\n",
      "Epoch 1 Batch 1086 Loss 1.6933\n",
      "Epoch 1 Batch 1087 Loss 1.5052\n",
      "Epoch 1 Batch 1088 Loss 1.6130\n",
      "Epoch 1 Batch 1089 Loss 1.7433\n",
      "Epoch 1 Batch 1090 Loss 1.7501\n",
      "Epoch 1 Batch 1091 Loss 1.6070\n",
      "Epoch 1 Batch 1092 Loss 1.3266\n",
      "Epoch 1 Batch 1093 Loss 1.3537\n",
      "Epoch 1 Batch 1094 Loss 1.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1095 Loss 1.4756\n",
      "Epoch 1 Batch 1096 Loss 1.4148\n",
      "Epoch 1 Batch 1097 Loss 1.6297\n",
      "Epoch 1 Batch 1098 Loss 1.5864\n",
      "Epoch 1 Batch 1099 Loss 1.4334\n",
      "Epoch 1 Batch 1100 Loss 1.7354\n",
      "Epoch 1 Batch 1101 Loss 1.4663\n",
      "Epoch 1 Batch 1102 Loss 1.6042\n",
      "Epoch 1 Batch 1103 Loss 1.6190\n",
      "Epoch 1 Batch 1104 Loss 1.4914\n",
      "Epoch 1 Batch 1105 Loss 1.5165\n",
      "Epoch 1 Batch 1106 Loss 1.4340\n",
      "Epoch 1 Batch 1107 Loss 1.5142\n",
      "Epoch 1 Batch 1108 Loss 1.4327\n",
      "Epoch 1 Batch 1109 Loss 1.2712\n",
      "Epoch 1 Batch 1110 Loss 1.5235\n",
      "Epoch 1 Batch 1111 Loss 1.4212\n",
      "Epoch 1 Batch 1112 Loss 1.4525\n",
      "Epoch 1 Batch 1113 Loss 1.6437\n",
      "Epoch 1 Batch 1114 Loss 1.4529\n",
      "Epoch 1 Batch 1115 Loss 1.7428\n",
      "Epoch 1 Batch 1116 Loss 1.4260\n",
      "Epoch 1 Batch 1117 Loss 1.3023\n",
      "Epoch 1 Batch 1118 Loss 1.3537\n",
      "Epoch 1 Batch 1119 Loss 1.6001\n",
      "Epoch 1 Batch 1120 Loss 1.3592\n",
      "Epoch 1 Batch 1121 Loss 1.3792\n",
      "Epoch 1 Batch 1122 Loss 1.6438\n",
      "Epoch 1 Batch 1123 Loss 1.4134\n",
      "Epoch 1 Batch 1124 Loss 1.6039\n",
      "Epoch 1 Batch 1125 Loss 1.4304\n",
      "Epoch 1 Batch 1126 Loss 1.4532\n",
      "Epoch 1 Batch 1127 Loss 1.5890\n",
      "Epoch 1 Batch 1128 Loss 1.4405\n",
      "Epoch 1 Batch 1129 Loss 1.5222\n",
      "Epoch 1 Batch 1130 Loss 1.5558\n",
      "Epoch 1 Batch 1131 Loss 1.6028\n",
      "Epoch 1 Batch 1132 Loss 1.4080\n",
      "Epoch 1 Batch 1133 Loss 1.4784\n",
      "Epoch 1 Batch 1134 Loss 1.4033\n",
      "Epoch 1 Batch 1135 Loss 1.6864\n",
      "Epoch 1 Batch 1136 Loss 1.4264\n",
      "Epoch 1 Batch 1137 Loss 1.4985\n",
      "Epoch 1 Batch 1138 Loss 1.1672\n",
      "Epoch 1 Batch 1139 Loss 1.5365\n",
      "Epoch 1 Batch 1140 Loss 1.4189\n",
      "Epoch 1 Batch 1141 Loss 1.5576\n",
      "Epoch 1 Batch 1142 Loss 1.5563\n",
      "Epoch 1 Batch 1143 Loss 1.5523\n",
      "Epoch 1 Batch 1144 Loss 1.4314\n",
      "Epoch 1 Batch 1145 Loss 1.7211\n",
      "Epoch 1 Batch 1146 Loss 1.6004\n",
      "Epoch 1 Batch 1147 Loss 1.4814\n",
      "Epoch 1 Batch 1148 Loss 1.5401\n",
      "Epoch 1 Batch 1149 Loss 1.5785\n",
      "Epoch 1 Batch 1150 Loss 1.6534\n",
      "Epoch 1 Batch 1151 Loss 1.6475\n",
      "Epoch 1 Batch 1152 Loss 1.5434\n",
      "Epoch 1 Batch 1153 Loss 1.2909\n",
      "Epoch 1 Batch 1154 Loss 1.4742\n",
      "Epoch 1 Batch 1155 Loss 1.6837\n",
      "Epoch 1 Batch 1156 Loss 1.5369\n",
      "Epoch 1 Batch 1157 Loss 1.5967\n",
      "Epoch 1 Batch 1158 Loss 1.6655\n",
      "Epoch 1 Batch 1159 Loss 1.6345\n",
      "Epoch 1 Batch 1160 Loss 1.4216\n",
      "Epoch 1 Batch 1161 Loss 1.4759\n",
      "Epoch 1 Batch 1162 Loss 1.3310\n",
      "Epoch 1 Batch 1163 Loss 1.8036\n",
      "Epoch 1 Batch 1164 Loss 1.6643\n",
      "Epoch 1 Batch 1165 Loss 1.3698\n",
      "Epoch 1 Batch 1166 Loss 1.3771\n",
      "Epoch 1 Batch 1167 Loss 1.6599\n",
      "Epoch 1 Batch 1168 Loss 1.4909\n",
      "Epoch 1 Batch 1169 Loss 1.4678\n",
      "Epoch 1 Batch 1170 Loss 1.5914\n",
      "Epoch 1 Batch 1171 Loss 1.3207\n",
      "Epoch 1 Batch 1172 Loss 1.6944\n",
      "Epoch 1 Batch 1173 Loss 1.4904\n",
      "Epoch 1 Batch 1174 Loss 1.5711\n",
      "Epoch 1 Batch 1175 Loss 1.5854\n",
      "Epoch 1 Batch 1176 Loss 1.6433\n",
      "Epoch 1 Batch 1177 Loss 1.5122\n",
      "Epoch 1 Batch 1178 Loss 1.4633\n",
      "Epoch 1 Batch 1179 Loss 1.5909\n",
      "Epoch 1 Batch 1180 Loss 1.3715\n",
      "Epoch 1 Batch 1181 Loss 1.4897\n",
      "Epoch 1 Batch 1182 Loss 1.3893\n",
      "Epoch 1 Batch 1183 Loss 1.5800\n",
      "Epoch 1 Batch 1184 Loss 1.3740\n",
      "Epoch 1 Batch 1185 Loss 1.4251\n",
      "Epoch 1 Batch 1186 Loss 1.5591\n",
      "Epoch 1 Batch 1187 Loss 1.2518\n",
      "Epoch 1 Batch 1188 Loss 1.5755\n",
      "Epoch 1 Batch 1189 Loss 1.3662\n",
      "Epoch 1 Batch 1190 Loss 1.4716\n",
      "Epoch 1 Batch 1191 Loss 1.4128\n",
      "Epoch 1 Batch 1192 Loss 1.2456\n",
      "Epoch 1 Batch 1193 Loss 1.5335\n",
      "Epoch 1 Batch 1194 Loss 1.5403\n",
      "Epoch 1 Batch 1195 Loss 1.5258\n",
      "Epoch 1 Batch 1196 Loss 1.6678\n",
      "Epoch 1 Batch 1197 Loss 1.5117\n",
      "Epoch 1 Batch 1198 Loss 1.5079\n",
      "Epoch 1 Batch 1199 Loss 1.4080\n",
      "Epoch 1 Batch 1200 Loss 1.5576\n",
      "Epoch 1 Batch 1201 Loss 1.7423\n",
      "Epoch 1 Batch 1202 Loss 1.3897\n",
      "Epoch 1 Batch 1203 Loss 1.3166\n",
      "Epoch 1 Batch 1204 Loss 1.4773\n",
      "Epoch 1 Batch 1205 Loss 1.6442\n",
      "Epoch 1 Batch 1206 Loss 1.5969\n",
      "Epoch 1 Batch 1207 Loss 1.4363\n",
      "Epoch 1 Batch 1208 Loss 1.4959\n",
      "Epoch 1 Batch 1209 Loss 1.5503\n",
      "Epoch 1 Batch 1210 Loss 1.4926\n",
      "Epoch 1 Batch 1211 Loss 1.2359\n",
      "Epoch 1 Batch 1212 Loss 1.5290\n",
      "Epoch 1 Batch 1213 Loss 1.4494\n",
      "Epoch 1 Batch 1214 Loss 1.5441\n",
      "Epoch 1 Batch 1215 Loss 1.6076\n",
      "Epoch 1 Batch 1216 Loss 1.4930\n",
      "Epoch 1 Batch 1217 Loss 1.5190\n",
      "Epoch 1 Batch 1218 Loss 1.8363\n",
      "Epoch 1 Batch 1219 Loss 1.4791\n",
      "Epoch 1 Batch 1220 Loss 1.4853\n",
      "Epoch 1 Batch 1221 Loss 1.5889\n",
      "Epoch 1 Batch 1222 Loss 1.6188\n",
      "Epoch 1 Batch 1223 Loss 1.5230\n",
      "Epoch 1 Batch 1224 Loss 1.4720\n",
      "Epoch 1 Batch 1225 Loss 1.4984\n",
      "Epoch 1 Batch 1226 Loss 1.3948\n",
      "Epoch 1 Batch 1227 Loss 1.6494\n",
      "Epoch 1 Batch 1228 Loss 1.5930\n",
      "Epoch 1 Batch 1229 Loss 1.4637\n",
      "Epoch 1 Batch 1230 Loss 1.3512\n",
      "Epoch 1 Batch 1231 Loss 1.6588\n",
      "Epoch 1 Batch 1232 Loss 1.4877\n",
      "Epoch 1 Batch 1233 Loss 1.2836\n",
      "Epoch 1 Batch 1234 Loss 1.4785\n",
      "Epoch 1 Batch 1235 Loss 1.4219\n",
      "Epoch 1 Batch 1236 Loss 1.5361\n",
      "Epoch 1 Batch 1237 Loss 1.5725\n",
      "Epoch 1 Batch 1238 Loss 1.6999\n",
      "Epoch 1 Batch 1239 Loss 1.4689\n",
      "Epoch 1 Batch 1240 Loss 1.7593\n",
      "Epoch 1 Batch 1241 Loss 1.3655\n",
      "Epoch 1 Batch 1242 Loss 1.3850\n",
      "Epoch 1 Batch 1243 Loss 1.4350\n",
      "Epoch 1 Batch 1244 Loss 1.5324\n",
      "Epoch 1 Batch 1245 Loss 1.5902\n",
      "Epoch 1 Batch 1246 Loss 1.2592\n",
      "Epoch 1 Batch 1247 Loss 1.3471\n",
      "Epoch 1 Batch 1248 Loss 1.5309\n",
      "Epoch 1 Batch 1249 Loss 1.5827\n",
      "Epoch 1 Batch 1250 Loss 1.6396\n",
      "Epoch 1 Batch 1251 Loss 1.5559\n",
      "Epoch 1 Batch 1252 Loss 1.4379\n",
      "Epoch 1 Batch 1253 Loss 1.4790\n",
      "Epoch 1 Batch 1254 Loss 1.3225\n",
      "Epoch 1 Batch 1255 Loss 1.4672\n",
      "Epoch 1 Batch 1256 Loss 1.7039\n",
      "Epoch 1 Batch 1257 Loss 1.4413\n",
      "Epoch 1 Batch 1258 Loss 1.7579\n",
      "Epoch 1 Batch 1259 Loss 1.5887\n",
      "Epoch 1 Batch 1260 Loss 1.5206\n",
      "Epoch 1 Batch 1261 Loss 1.5929\n",
      "Epoch 1 Batch 1262 Loss 1.3790\n",
      "Epoch 1 Batch 1263 Loss 1.5583\n",
      "Epoch 1 Batch 1264 Loss 1.3751\n",
      "Epoch 1 Batch 1265 Loss 1.4849\n",
      "Epoch 1 Batch 1266 Loss 1.7148\n",
      "Epoch 1 Batch 1267 Loss 1.5443\n",
      "Epoch 1 Batch 1268 Loss 1.4700\n",
      "Epoch 1 Batch 1269 Loss 1.4878\n",
      "Epoch 1 Batch 1270 Loss 1.5455\n",
      "Epoch 1 Batch 1271 Loss 1.3615\n",
      "Epoch 1 Batch 1272 Loss 1.7534\n",
      "Epoch 1 Batch 1273 Loss 1.3712\n",
      "Epoch 1 Batch 1274 Loss 1.3644\n",
      "Epoch 1 Batch 1275 Loss 1.5462\n",
      "Epoch 1 Batch 1276 Loss 1.5624\n",
      "Epoch 1 Batch 1277 Loss 1.6856\n",
      "Epoch 1 Batch 1278 Loss 1.3569\n",
      "Epoch 1 Batch 1279 Loss 1.4464\n",
      "Epoch 1 Batch 1280 Loss 1.4548\n",
      "Epoch 1 Batch 1281 Loss 1.8475\n",
      "Epoch 1 Batch 1282 Loss 1.5044\n",
      "Epoch 1 Batch 1283 Loss 1.5327\n",
      "Epoch 1 Batch 1284 Loss 1.6475\n",
      "Epoch 1 Batch 1285 Loss 1.3045\n",
      "Epoch 1 Batch 1286 Loss 1.4626\n",
      "Epoch 1 Batch 1287 Loss 1.5856\n",
      "Epoch 1 Batch 1288 Loss 1.3310\n",
      "Epoch 1 Batch 1289 Loss 1.6247\n",
      "Epoch 1 Batch 1290 Loss 1.5694\n",
      "Epoch 1 Batch 1291 Loss 1.5941\n",
      "Epoch 1 Batch 1292 Loss 1.6324\n",
      "Epoch 1 Batch 1293 Loss 1.3683\n",
      "Epoch 1 Loss 1.6269\n",
      "Time taken for 1 epoch 397.9800617694855 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5747\n",
      "Epoch 2 Batch 1 Loss 1.3843\n",
      "Epoch 2 Batch 2 Loss 1.4341\n",
      "Epoch 2 Batch 3 Loss 1.4769\n",
      "Epoch 2 Batch 4 Loss 1.3623\n",
      "Epoch 2 Batch 5 Loss 1.5881\n",
      "Epoch 2 Batch 6 Loss 1.5477\n",
      "Epoch 2 Batch 7 Loss 1.6021\n",
      "Epoch 2 Batch 8 Loss 1.6049\n",
      "Epoch 2 Batch 9 Loss 1.4266\n",
      "Epoch 2 Batch 10 Loss 1.4099\n",
      "Epoch 2 Batch 11 Loss 1.4541\n",
      "Epoch 2 Batch 12 Loss 1.4676\n",
      "Epoch 2 Batch 13 Loss 1.2643\n",
      "Epoch 2 Batch 14 Loss 1.4145\n",
      "Epoch 2 Batch 15 Loss 1.5277\n",
      "Epoch 2 Batch 16 Loss 1.2221\n",
      "Epoch 2 Batch 17 Loss 1.2848\n",
      "Epoch 2 Batch 18 Loss 1.2940\n",
      "Epoch 2 Batch 19 Loss 1.3575\n",
      "Epoch 2 Batch 20 Loss 1.3625\n",
      "Epoch 2 Batch 21 Loss 1.3261\n",
      "Epoch 2 Batch 22 Loss 1.5458\n",
      "Epoch 2 Batch 23 Loss 1.5042\n",
      "Epoch 2 Batch 24 Loss 1.5445\n",
      "Epoch 2 Batch 25 Loss 1.3102\n",
      "Epoch 2 Batch 26 Loss 1.4343\n",
      "Epoch 2 Batch 27 Loss 1.4859\n",
      "Epoch 2 Batch 28 Loss 1.5891\n",
      "Epoch 2 Batch 29 Loss 1.6042\n",
      "Epoch 2 Batch 30 Loss 1.3984\n",
      "Epoch 2 Batch 31 Loss 1.4122\n",
      "Epoch 2 Batch 32 Loss 1.2642\n",
      "Epoch 2 Batch 33 Loss 1.3473\n",
      "Epoch 2 Batch 34 Loss 1.2390\n",
      "Epoch 2 Batch 35 Loss 1.5981\n",
      "Epoch 2 Batch 36 Loss 1.3076\n",
      "Epoch 2 Batch 37 Loss 1.5518\n",
      "Epoch 2 Batch 38 Loss 1.4186\n",
      "Epoch 2 Batch 39 Loss 1.3963\n",
      "Epoch 2 Batch 40 Loss 1.4490\n",
      "Epoch 2 Batch 41 Loss 1.2737\n",
      "Epoch 2 Batch 42 Loss 1.5060\n",
      "Epoch 2 Batch 43 Loss 1.4129\n",
      "Epoch 2 Batch 44 Loss 1.4522\n",
      "Epoch 2 Batch 45 Loss 1.5176\n",
      "Epoch 2 Batch 46 Loss 1.2297\n",
      "Epoch 2 Batch 47 Loss 1.4676\n",
      "Epoch 2 Batch 48 Loss 1.6278\n",
      "Epoch 2 Batch 49 Loss 1.2154\n",
      "Epoch 2 Batch 50 Loss 1.5780\n",
      "Epoch 2 Batch 51 Loss 1.3766\n",
      "Epoch 2 Batch 52 Loss 1.3374\n",
      "Epoch 2 Batch 53 Loss 1.4218\n",
      "Epoch 2 Batch 54 Loss 1.2383\n",
      "Epoch 2 Batch 55 Loss 1.2870\n",
      "Epoch 2 Batch 56 Loss 1.3401\n",
      "Epoch 2 Batch 57 Loss 1.5535\n",
      "Epoch 2 Batch 58 Loss 1.3751\n",
      "Epoch 2 Batch 59 Loss 1.5559\n",
      "Epoch 2 Batch 60 Loss 1.2097\n",
      "Epoch 2 Batch 61 Loss 1.4622\n",
      "Epoch 2 Batch 62 Loss 1.4825\n",
      "Epoch 2 Batch 63 Loss 1.4573\n",
      "Epoch 2 Batch 64 Loss 1.4111\n",
      "Epoch 2 Batch 65 Loss 1.5282\n",
      "Epoch 2 Batch 66 Loss 1.2352\n",
      "Epoch 2 Batch 67 Loss 1.4097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 68 Loss 1.4420\n",
      "Epoch 2 Batch 69 Loss 1.5385\n",
      "Epoch 2 Batch 70 Loss 1.5093\n",
      "Epoch 2 Batch 71 Loss 1.6350\n",
      "Epoch 2 Batch 72 Loss 1.4212\n",
      "Epoch 2 Batch 73 Loss 1.6297\n",
      "Epoch 2 Batch 74 Loss 1.5176\n",
      "Epoch 2 Batch 75 Loss 1.4265\n",
      "Epoch 2 Batch 76 Loss 1.1761\n",
      "Epoch 2 Batch 77 Loss 1.5902\n",
      "Epoch 2 Batch 78 Loss 1.4935\n",
      "Epoch 2 Batch 79 Loss 1.2943\n",
      "Epoch 2 Batch 80 Loss 1.4566\n",
      "Epoch 2 Batch 81 Loss 1.6185\n",
      "Epoch 2 Batch 82 Loss 1.2738\n",
      "Epoch 2 Batch 83 Loss 1.4734\n",
      "Epoch 2 Batch 84 Loss 1.5484\n",
      "Epoch 2 Batch 85 Loss 1.3567\n",
      "Epoch 2 Batch 86 Loss 1.3336\n",
      "Epoch 2 Batch 87 Loss 1.3789\n",
      "Epoch 2 Batch 88 Loss 1.3108\n",
      "Epoch 2 Batch 89 Loss 1.3570\n",
      "Epoch 2 Batch 90 Loss 1.5692\n",
      "Epoch 2 Batch 91 Loss 1.4831\n",
      "Epoch 2 Batch 92 Loss 1.1933\n",
      "Epoch 2 Batch 93 Loss 1.5167\n",
      "Epoch 2 Batch 94 Loss 1.2944\n",
      "Epoch 2 Batch 95 Loss 1.4659\n",
      "Epoch 2 Batch 96 Loss 1.6358\n",
      "Epoch 2 Batch 97 Loss 1.2953\n",
      "Epoch 2 Batch 98 Loss 1.4317\n",
      "Epoch 2 Batch 99 Loss 1.3814\n",
      "Epoch 2 Batch 100 Loss 1.3330\n",
      "Epoch 2 Batch 101 Loss 1.4123\n",
      "Epoch 2 Batch 102 Loss 1.3199\n",
      "Epoch 2 Batch 103 Loss 1.4640\n",
      "Epoch 2 Batch 104 Loss 1.5897\n",
      "Epoch 2 Batch 105 Loss 1.5520\n",
      "Epoch 2 Batch 106 Loss 1.3485\n",
      "Epoch 2 Batch 107 Loss 1.4016\n",
      "Epoch 2 Batch 108 Loss 1.5978\n",
      "Epoch 2 Batch 109 Loss 1.4311\n",
      "Epoch 2 Batch 110 Loss 1.3329\n",
      "Epoch 2 Batch 111 Loss 1.3212\n",
      "Epoch 2 Batch 112 Loss 1.5424\n",
      "Epoch 2 Batch 113 Loss 1.3983\n",
      "Epoch 2 Batch 114 Loss 1.3795\n",
      "Epoch 2 Batch 115 Loss 1.3335\n",
      "Epoch 2 Batch 116 Loss 1.2815\n",
      "Epoch 2 Batch 117 Loss 1.3899\n",
      "Epoch 2 Batch 118 Loss 1.3269\n",
      "Epoch 2 Batch 119 Loss 1.5644\n",
      "Epoch 2 Batch 120 Loss 1.3842\n",
      "Epoch 2 Batch 121 Loss 1.5071\n",
      "Epoch 2 Batch 122 Loss 1.4035\n",
      "Epoch 2 Batch 123 Loss 1.3451\n",
      "Epoch 2 Batch 124 Loss 1.4511\n",
      "Epoch 2 Batch 125 Loss 1.3884\n",
      "Epoch 2 Batch 126 Loss 1.4384\n",
      "Epoch 2 Batch 127 Loss 1.4487\n",
      "Epoch 2 Batch 128 Loss 1.4775\n",
      "Epoch 2 Batch 129 Loss 1.4148\n",
      "Epoch 2 Batch 130 Loss 1.3742\n",
      "Epoch 2 Batch 131 Loss 1.2852\n",
      "Epoch 2 Batch 132 Loss 1.2197\n",
      "Epoch 2 Batch 133 Loss 1.4674\n",
      "Epoch 2 Batch 134 Loss 1.4720\n",
      "Epoch 2 Batch 135 Loss 1.3631\n",
      "Epoch 2 Batch 136 Loss 1.3720\n",
      "Epoch 2 Batch 137 Loss 1.4995\n",
      "Epoch 2 Batch 138 Loss 1.4155\n",
      "Epoch 2 Batch 139 Loss 1.5016\n",
      "Epoch 2 Batch 140 Loss 1.3628\n",
      "Epoch 2 Batch 141 Loss 1.5822\n",
      "Epoch 2 Batch 142 Loss 1.3853\n",
      "Epoch 2 Batch 143 Loss 1.5224\n",
      "Epoch 2 Batch 144 Loss 1.2833\n",
      "Epoch 2 Batch 145 Loss 1.2878\n",
      "Epoch 2 Batch 146 Loss 1.4454\n",
      "Epoch 2 Batch 147 Loss 1.5639\n",
      "Epoch 2 Batch 148 Loss 1.4192\n",
      "Epoch 2 Batch 149 Loss 1.5736\n",
      "Epoch 2 Batch 150 Loss 1.3433\n",
      "Epoch 2 Batch 151 Loss 1.3292\n",
      "Epoch 2 Batch 152 Loss 1.6639\n",
      "Epoch 2 Batch 153 Loss 1.5630\n",
      "Epoch 2 Batch 154 Loss 1.2978\n",
      "Epoch 2 Batch 155 Loss 1.3379\n",
      "Epoch 2 Batch 156 Loss 1.5092\n",
      "Epoch 2 Batch 157 Loss 1.2786\n",
      "Epoch 2 Batch 158 Loss 1.6544\n",
      "Epoch 2 Batch 159 Loss 1.3441\n",
      "Epoch 2 Batch 160 Loss 1.4651\n",
      "Epoch 2 Batch 161 Loss 1.4416\n",
      "Epoch 2 Batch 162 Loss 1.4441\n",
      "Epoch 2 Batch 163 Loss 1.5671\n",
      "Epoch 2 Batch 164 Loss 1.1913\n",
      "Epoch 2 Batch 165 Loss 1.4773\n",
      "Epoch 2 Batch 166 Loss 1.3699\n",
      "Epoch 2 Batch 167 Loss 1.6136\n",
      "Epoch 2 Batch 168 Loss 1.4397\n",
      "Epoch 2 Batch 169 Loss 1.5217\n",
      "Epoch 2 Batch 170 Loss 1.4499\n",
      "Epoch 2 Batch 171 Loss 1.3291\n",
      "Epoch 2 Batch 172 Loss 1.2922\n",
      "Epoch 2 Batch 173 Loss 1.3723\n",
      "Epoch 2 Batch 174 Loss 1.3696\n",
      "Epoch 2 Batch 175 Loss 1.3861\n",
      "Epoch 2 Batch 176 Loss 1.4403\n",
      "Epoch 2 Batch 177 Loss 1.3650\n",
      "Epoch 2 Batch 178 Loss 1.3776\n",
      "Epoch 2 Batch 179 Loss 1.5912\n",
      "Epoch 2 Batch 180 Loss 1.3712\n",
      "Epoch 2 Batch 181 Loss 1.3589\n",
      "Epoch 2 Batch 182 Loss 1.5126\n",
      "Epoch 2 Batch 183 Loss 1.4830\n",
      "Epoch 2 Batch 184 Loss 1.4261\n",
      "Epoch 2 Batch 185 Loss 1.6033\n",
      "Epoch 2 Batch 186 Loss 1.3722\n",
      "Epoch 2 Batch 187 Loss 1.4945\n",
      "Epoch 2 Batch 188 Loss 1.4179\n",
      "Epoch 2 Batch 189 Loss 1.3609\n",
      "Epoch 2 Batch 190 Loss 1.7640\n",
      "Epoch 2 Batch 191 Loss 1.4220\n",
      "Epoch 2 Batch 192 Loss 1.4873\n",
      "Epoch 2 Batch 193 Loss 1.4027\n",
      "Epoch 2 Batch 194 Loss 1.3455\n",
      "Epoch 2 Batch 195 Loss 1.6361\n",
      "Epoch 2 Batch 196 Loss 1.5320\n",
      "Epoch 2 Batch 197 Loss 1.4084\n",
      "Epoch 2 Batch 198 Loss 1.2047\n",
      "Epoch 2 Batch 199 Loss 1.6511\n",
      "Epoch 2 Batch 200 Loss 1.3937\n",
      "Epoch 2 Batch 201 Loss 1.6269\n",
      "Epoch 2 Batch 202 Loss 1.3399\n",
      "Epoch 2 Batch 203 Loss 1.2922\n",
      "Epoch 2 Batch 204 Loss 1.3945\n",
      "Epoch 2 Batch 205 Loss 1.2496\n",
      "Epoch 2 Batch 206 Loss 1.4324\n",
      "Epoch 2 Batch 207 Loss 1.2447\n",
      "Epoch 2 Batch 208 Loss 1.3271\n",
      "Epoch 2 Batch 209 Loss 1.3160\n",
      "Epoch 2 Batch 210 Loss 1.6823\n",
      "Epoch 2 Batch 211 Loss 1.6930\n",
      "Epoch 2 Batch 212 Loss 1.4026\n",
      "Epoch 2 Batch 213 Loss 1.2925\n",
      "Epoch 2 Batch 214 Loss 1.4804\n",
      "Epoch 2 Batch 215 Loss 1.4422\n",
      "Epoch 2 Batch 216 Loss 1.5568\n",
      "Epoch 2 Batch 217 Loss 1.2442\n",
      "Epoch 2 Batch 218 Loss 1.2943\n",
      "Epoch 2 Batch 219 Loss 1.5063\n",
      "Epoch 2 Batch 220 Loss 1.5002\n",
      "Epoch 2 Batch 221 Loss 1.6416\n",
      "Epoch 2 Batch 222 Loss 1.4911\n",
      "Epoch 2 Batch 223 Loss 1.3395\n",
      "Epoch 2 Batch 224 Loss 1.3652\n",
      "Epoch 2 Batch 225 Loss 1.2754\n",
      "Epoch 2 Batch 226 Loss 1.2924\n",
      "Epoch 2 Batch 227 Loss 1.6468\n",
      "Epoch 2 Batch 228 Loss 1.7021\n",
      "Epoch 2 Batch 229 Loss 1.2842\n",
      "Epoch 2 Batch 230 Loss 1.2692\n",
      "Epoch 2 Batch 231 Loss 1.3790\n",
      "Epoch 2 Batch 232 Loss 1.5125\n",
      "Epoch 2 Batch 233 Loss 1.3791\n",
      "Epoch 2 Batch 234 Loss 1.3113\n",
      "Epoch 2 Batch 235 Loss 1.4699\n",
      "Epoch 2 Batch 236 Loss 1.2550\n",
      "Epoch 2 Batch 237 Loss 1.4412\n",
      "Epoch 2 Batch 238 Loss 1.4371\n",
      "Epoch 2 Batch 239 Loss 1.6309\n",
      "Epoch 2 Batch 240 Loss 1.4524\n",
      "Epoch 2 Batch 241 Loss 1.2153\n",
      "Epoch 2 Batch 242 Loss 1.4094\n",
      "Epoch 2 Batch 243 Loss 1.4317\n",
      "Epoch 2 Batch 244 Loss 1.5303\n",
      "Epoch 2 Batch 245 Loss 1.5386\n",
      "Epoch 2 Batch 246 Loss 1.4841\n",
      "Epoch 2 Batch 247 Loss 1.4558\n",
      "Epoch 2 Batch 248 Loss 1.5175\n",
      "Epoch 2 Batch 249 Loss 1.4589\n",
      "Epoch 2 Batch 250 Loss 1.3954\n",
      "Epoch 2 Batch 251 Loss 1.5757\n",
      "Epoch 2 Batch 252 Loss 1.5954\n",
      "Epoch 2 Batch 253 Loss 1.3776\n",
      "Epoch 2 Batch 254 Loss 1.4903\n",
      "Epoch 2 Batch 255 Loss 1.3081\n",
      "Epoch 2 Batch 256 Loss 1.2835\n",
      "Epoch 2 Batch 257 Loss 1.3624\n",
      "Epoch 2 Batch 258 Loss 1.2953\n",
      "Epoch 2 Batch 259 Loss 1.4265\n",
      "Epoch 2 Batch 260 Loss 1.6559\n",
      "Epoch 2 Batch 261 Loss 1.4485\n",
      "Epoch 2 Batch 262 Loss 1.4756\n",
      "Epoch 2 Batch 263 Loss 1.5228\n",
      "Epoch 2 Batch 264 Loss 1.4955\n",
      "Epoch 2 Batch 265 Loss 1.3715\n",
      "Epoch 2 Batch 266 Loss 1.4912\n",
      "Epoch 2 Batch 267 Loss 1.2226\n",
      "Epoch 2 Batch 268 Loss 1.3094\n",
      "Epoch 2 Batch 269 Loss 1.3281\n",
      "Epoch 2 Batch 270 Loss 1.5052\n",
      "Epoch 2 Batch 271 Loss 1.8436\n",
      "Epoch 2 Batch 272 Loss 1.2842\n",
      "Epoch 2 Batch 273 Loss 1.3985\n",
      "Epoch 2 Batch 274 Loss 1.2945\n",
      "Epoch 2 Batch 275 Loss 1.3483\n",
      "Epoch 2 Batch 276 Loss 1.3930\n",
      "Epoch 2 Batch 277 Loss 1.3567\n",
      "Epoch 2 Batch 278 Loss 1.5565\n",
      "Epoch 2 Batch 279 Loss 1.7356\n",
      "Epoch 2 Batch 280 Loss 1.4707\n",
      "Epoch 2 Batch 281 Loss 1.3356\n",
      "Epoch 2 Batch 282 Loss 1.6581\n",
      "Epoch 2 Batch 283 Loss 1.4810\n",
      "Epoch 2 Batch 284 Loss 1.4744\n",
      "Epoch 2 Batch 285 Loss 1.4004\n",
      "Epoch 2 Batch 286 Loss 1.5246\n",
      "Epoch 2 Batch 287 Loss 1.0909\n",
      "Epoch 2 Batch 288 Loss 1.4970\n",
      "Epoch 2 Batch 289 Loss 1.4501\n",
      "Epoch 2 Batch 290 Loss 1.4040\n",
      "Epoch 2 Batch 291 Loss 1.3081\n",
      "Epoch 2 Batch 292 Loss 1.4227\n",
      "Epoch 2 Batch 293 Loss 1.4747\n",
      "Epoch 2 Batch 294 Loss 1.5566\n",
      "Epoch 2 Batch 295 Loss 1.3691\n",
      "Epoch 2 Batch 296 Loss 1.3660\n",
      "Epoch 2 Batch 297 Loss 1.4884\n",
      "Epoch 2 Batch 298 Loss 1.3162\n",
      "Epoch 2 Batch 299 Loss 1.4487\n",
      "Epoch 2 Batch 300 Loss 1.5338\n",
      "Epoch 2 Batch 301 Loss 1.4413\n",
      "Epoch 2 Batch 302 Loss 1.8545\n",
      "Epoch 2 Batch 303 Loss 1.4008\n",
      "Epoch 2 Batch 304 Loss 1.4835\n",
      "Epoch 2 Batch 305 Loss 1.5493\n",
      "Epoch 2 Batch 306 Loss 1.5122\n",
      "Epoch 2 Batch 307 Loss 1.3110\n",
      "Epoch 2 Batch 308 Loss 1.3173\n",
      "Epoch 2 Batch 309 Loss 1.4655\n",
      "Epoch 2 Batch 310 Loss 1.2670\n",
      "Epoch 2 Batch 311 Loss 1.5794\n",
      "Epoch 2 Batch 312 Loss 1.7698\n",
      "Epoch 2 Batch 313 Loss 1.4004\n",
      "Epoch 2 Batch 314 Loss 1.2278\n",
      "Epoch 2 Batch 315 Loss 1.3695\n",
      "Epoch 2 Batch 316 Loss 1.4799\n",
      "Epoch 2 Batch 317 Loss 1.3106\n",
      "Epoch 2 Batch 318 Loss 1.6108\n",
      "Epoch 2 Batch 319 Loss 1.2552\n",
      "Epoch 2 Batch 320 Loss 1.4324\n",
      "Epoch 2 Batch 321 Loss 1.3161\n",
      "Epoch 2 Batch 322 Loss 1.2816\n",
      "Epoch 2 Batch 323 Loss 1.3712\n",
      "Epoch 2 Batch 324 Loss 1.4492\n",
      "Epoch 2 Batch 325 Loss 1.2700\n",
      "Epoch 2 Batch 326 Loss 1.3681\n",
      "Epoch 2 Batch 327 Loss 1.7992\n",
      "Epoch 2 Batch 328 Loss 1.6248\n",
      "Epoch 2 Batch 329 Loss 1.3872\n",
      "Epoch 2 Batch 330 Loss 1.1821\n",
      "Epoch 2 Batch 331 Loss 1.3827\n",
      "Epoch 2 Batch 332 Loss 1.5320\n",
      "Epoch 2 Batch 333 Loss 1.2686\n",
      "Epoch 2 Batch 334 Loss 1.3744\n",
      "Epoch 2 Batch 335 Loss 1.5654\n",
      "Epoch 2 Batch 336 Loss 1.3334\n",
      "Epoch 2 Batch 337 Loss 1.3758\n",
      "Epoch 2 Batch 338 Loss 1.4328\n",
      "Epoch 2 Batch 339 Loss 1.3059\n",
      "Epoch 2 Batch 340 Loss 1.1153\n",
      "Epoch 2 Batch 341 Loss 1.1989\n",
      "Epoch 2 Batch 342 Loss 1.2771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 343 Loss 1.2210\n",
      "Epoch 2 Batch 344 Loss 1.2740\n",
      "Epoch 2 Batch 345 Loss 1.1542\n",
      "Epoch 2 Batch 346 Loss 1.4041\n",
      "Epoch 2 Batch 347 Loss 1.2331\n",
      "Epoch 2 Batch 348 Loss 1.3722\n",
      "Epoch 2 Batch 349 Loss 1.5513\n",
      "Epoch 2 Batch 350 Loss 1.3968\n",
      "Epoch 2 Batch 351 Loss 1.4590\n",
      "Epoch 2 Batch 352 Loss 1.4707\n",
      "Epoch 2 Batch 353 Loss 1.5614\n",
      "Epoch 2 Batch 354 Loss 1.2292\n",
      "Epoch 2 Batch 355 Loss 1.4470\n",
      "Epoch 2 Batch 356 Loss 1.3822\n",
      "Epoch 2 Batch 357 Loss 1.2951\n",
      "Epoch 2 Batch 358 Loss 1.6244\n",
      "Epoch 2 Batch 359 Loss 1.2529\n",
      "Epoch 2 Batch 360 Loss 1.3845\n",
      "Epoch 2 Batch 361 Loss 1.4428\n",
      "Epoch 2 Batch 362 Loss 1.4730\n",
      "Epoch 2 Batch 363 Loss 1.2701\n",
      "Epoch 2 Batch 364 Loss 1.2484\n",
      "Epoch 2 Batch 365 Loss 1.3773\n",
      "Epoch 2 Batch 366 Loss 1.2236\n",
      "Epoch 2 Batch 367 Loss 1.3907\n",
      "Epoch 2 Batch 368 Loss 1.3465\n",
      "Epoch 2 Batch 369 Loss 1.5141\n",
      "Epoch 2 Batch 370 Loss 1.2787\n",
      "Epoch 2 Batch 371 Loss 1.5089\n",
      "Epoch 2 Batch 372 Loss 1.4852\n",
      "Epoch 2 Batch 373 Loss 1.0943\n",
      "Epoch 2 Batch 374 Loss 1.4015\n",
      "Epoch 2 Batch 375 Loss 1.4219\n",
      "Epoch 2 Batch 376 Loss 1.4415\n",
      "Epoch 2 Batch 377 Loss 1.7711\n",
      "Epoch 2 Batch 378 Loss 1.5847\n",
      "Epoch 2 Batch 379 Loss 1.1416\n",
      "Epoch 2 Batch 380 Loss 1.3794\n",
      "Epoch 2 Batch 381 Loss 1.4768\n",
      "Epoch 2 Batch 382 Loss 1.4472\n",
      "Epoch 2 Batch 383 Loss 1.3434\n",
      "Epoch 2 Batch 384 Loss 1.4000\n",
      "Epoch 2 Batch 385 Loss 1.3000\n",
      "Epoch 2 Batch 386 Loss 1.3356\n",
      "Epoch 2 Batch 387 Loss 1.4982\n",
      "Epoch 2 Batch 388 Loss 1.7431\n",
      "Epoch 2 Batch 389 Loss 1.3053\n",
      "Epoch 2 Batch 390 Loss 1.6233\n",
      "Epoch 2 Batch 391 Loss 1.1756\n",
      "Epoch 2 Batch 392 Loss 1.4158\n",
      "Epoch 2 Batch 393 Loss 1.6139\n",
      "Epoch 2 Batch 394 Loss 1.2425\n",
      "Epoch 2 Batch 395 Loss 1.4423\n",
      "Epoch 2 Batch 396 Loss 1.3699\n",
      "Epoch 2 Batch 397 Loss 1.6131\n",
      "Epoch 2 Batch 398 Loss 1.5642\n",
      "Epoch 2 Batch 399 Loss 1.3544\n",
      "Epoch 2 Batch 400 Loss 1.3123\n",
      "Epoch 2 Batch 401 Loss 1.5459\n",
      "Epoch 2 Batch 402 Loss 1.4737\n",
      "Epoch 2 Batch 403 Loss 1.4518\n",
      "Epoch 2 Batch 404 Loss 1.6009\n",
      "Epoch 2 Batch 405 Loss 1.4191\n",
      "Epoch 2 Batch 406 Loss 1.4754\n",
      "Epoch 2 Batch 407 Loss 1.4009\n",
      "Epoch 2 Batch 408 Loss 1.3101\n",
      "Epoch 2 Batch 409 Loss 1.2993\n",
      "Epoch 2 Batch 410 Loss 1.5011\n",
      "Epoch 2 Batch 411 Loss 1.2908\n",
      "Epoch 2 Batch 412 Loss 1.4120\n",
      "Epoch 2 Batch 413 Loss 1.3161\n",
      "Epoch 2 Batch 414 Loss 1.4137\n",
      "Epoch 2 Batch 415 Loss 1.5468\n",
      "Epoch 2 Batch 416 Loss 1.4864\n",
      "Epoch 2 Batch 417 Loss 1.2764\n",
      "Epoch 2 Batch 418 Loss 1.4050\n",
      "Epoch 2 Batch 419 Loss 1.3633\n",
      "Epoch 2 Batch 420 Loss 1.3451\n",
      "Epoch 2 Batch 421 Loss 1.5179\n",
      "Epoch 2 Batch 422 Loss 1.2810\n",
      "Epoch 2 Batch 423 Loss 1.4234\n",
      "Epoch 2 Batch 424 Loss 1.2738\n",
      "Epoch 2 Batch 425 Loss 1.3144\n",
      "Epoch 2 Batch 426 Loss 1.5295\n",
      "Epoch 2 Batch 427 Loss 1.2352\n",
      "Epoch 2 Batch 428 Loss 1.4731\n",
      "Epoch 2 Batch 429 Loss 1.3202\n",
      "Epoch 2 Batch 430 Loss 1.3688\n",
      "Epoch 2 Batch 431 Loss 1.5126\n",
      "Epoch 2 Batch 432 Loss 1.6355\n",
      "Epoch 2 Batch 433 Loss 1.3234\n",
      "Epoch 2 Batch 434 Loss 1.5105\n",
      "Epoch 2 Batch 435 Loss 1.2353\n",
      "Epoch 2 Batch 436 Loss 1.4285\n",
      "Epoch 2 Batch 437 Loss 1.3690\n",
      "Epoch 2 Batch 438 Loss 1.2617\n",
      "Epoch 2 Batch 439 Loss 1.1626\n",
      "Epoch 2 Batch 440 Loss 1.3829\n",
      "Epoch 2 Batch 441 Loss 1.3339\n",
      "Epoch 2 Batch 442 Loss 1.4163\n",
      "Epoch 2 Batch 443 Loss 1.2174\n",
      "Epoch 2 Batch 444 Loss 1.6589\n",
      "Epoch 2 Batch 445 Loss 1.4032\n",
      "Epoch 2 Batch 446 Loss 1.5651\n",
      "Epoch 2 Batch 447 Loss 1.4526\n",
      "Epoch 2 Batch 448 Loss 1.4001\n",
      "Epoch 2 Batch 449 Loss 1.3120\n",
      "Epoch 2 Batch 450 Loss 1.6018\n",
      "Epoch 2 Batch 451 Loss 1.1152\n",
      "Epoch 2 Batch 452 Loss 1.3188\n",
      "Epoch 2 Batch 453 Loss 1.3567\n",
      "Epoch 2 Batch 454 Loss 1.4207\n",
      "Epoch 2 Batch 455 Loss 1.3938\n",
      "Epoch 2 Batch 456 Loss 1.4279\n",
      "Epoch 2 Batch 457 Loss 1.3287\n",
      "Epoch 2 Batch 458 Loss 1.5478\n",
      "Epoch 2 Batch 459 Loss 1.3303\n",
      "Epoch 2 Batch 460 Loss 1.5131\n",
      "Epoch 2 Batch 461 Loss 1.2714\n",
      "Epoch 2 Batch 462 Loss 1.4254\n",
      "Epoch 2 Batch 463 Loss 1.4608\n",
      "Epoch 2 Batch 464 Loss 1.1840\n",
      "Epoch 2 Batch 465 Loss 1.3548\n",
      "Epoch 2 Batch 466 Loss 1.4468\n",
      "Epoch 2 Batch 467 Loss 1.3470\n",
      "Epoch 2 Batch 468 Loss 1.2551\n",
      "Epoch 2 Batch 469 Loss 1.5568\n",
      "Epoch 2 Batch 470 Loss 1.5047\n",
      "Epoch 2 Batch 471 Loss 1.3053\n",
      "Epoch 2 Batch 472 Loss 1.2412\n",
      "Epoch 2 Batch 473 Loss 1.4087\n",
      "Epoch 2 Batch 474 Loss 1.3468\n",
      "Epoch 2 Batch 475 Loss 1.4427\n",
      "Epoch 2 Batch 476 Loss 1.3120\n",
      "Epoch 2 Batch 477 Loss 1.4094\n",
      "Epoch 2 Batch 478 Loss 1.4983\n",
      "Epoch 2 Batch 479 Loss 1.3898\n",
      "Epoch 2 Batch 480 Loss 1.4252\n",
      "Epoch 2 Batch 481 Loss 1.1770\n",
      "Epoch 2 Batch 482 Loss 1.4128\n",
      "Epoch 2 Batch 483 Loss 1.3840\n",
      "Epoch 2 Batch 484 Loss 1.2096\n",
      "Epoch 2 Batch 485 Loss 1.3491\n",
      "Epoch 2 Batch 486 Loss 1.1152\n",
      "Epoch 2 Batch 487 Loss 1.2165\n",
      "Epoch 2 Batch 488 Loss 1.2828\n",
      "Epoch 2 Batch 489 Loss 1.3730\n",
      "Epoch 2 Batch 490 Loss 1.2995\n",
      "Epoch 2 Batch 491 Loss 1.2162\n",
      "Epoch 2 Batch 492 Loss 1.3505\n",
      "Epoch 2 Batch 493 Loss 1.2928\n",
      "Epoch 2 Batch 494 Loss 1.2674\n",
      "Epoch 2 Batch 495 Loss 1.5470\n",
      "Epoch 2 Batch 496 Loss 1.3870\n",
      "Epoch 2 Batch 497 Loss 1.5243\n",
      "Epoch 2 Batch 498 Loss 1.5079\n",
      "Epoch 2 Batch 499 Loss 1.4153\n",
      "Epoch 2 Batch 500 Loss 1.4765\n",
      "Epoch 2 Batch 501 Loss 1.6342\n",
      "Epoch 2 Batch 502 Loss 1.3014\n",
      "Epoch 2 Batch 503 Loss 1.3206\n",
      "Epoch 2 Batch 504 Loss 1.4708\n",
      "Epoch 2 Batch 505 Loss 1.2340\n",
      "Epoch 2 Batch 506 Loss 1.4083\n",
      "Epoch 2 Batch 507 Loss 1.3614\n",
      "Epoch 2 Batch 508 Loss 1.4777\n",
      "Epoch 2 Batch 509 Loss 1.3708\n",
      "Epoch 2 Batch 510 Loss 1.2932\n",
      "Epoch 2 Batch 511 Loss 1.3810\n",
      "Epoch 2 Batch 512 Loss 1.3812\n",
      "Epoch 2 Batch 513 Loss 1.5750\n",
      "Epoch 2 Batch 514 Loss 1.4908\n",
      "Epoch 2 Batch 515 Loss 1.2392\n",
      "Epoch 2 Batch 516 Loss 1.1866\n",
      "Epoch 2 Batch 517 Loss 1.4002\n",
      "Epoch 2 Batch 518 Loss 1.5408\n",
      "Epoch 2 Batch 519 Loss 1.2500\n",
      "Epoch 2 Batch 520 Loss 1.3777\n",
      "Epoch 2 Batch 521 Loss 1.3363\n",
      "Epoch 2 Batch 522 Loss 1.4738\n",
      "Epoch 2 Batch 523 Loss 1.5534\n",
      "Epoch 2 Batch 524 Loss 1.4340\n",
      "Epoch 2 Batch 525 Loss 1.4407\n",
      "Epoch 2 Batch 526 Loss 1.4347\n",
      "Epoch 2 Batch 527 Loss 1.3159\n",
      "Epoch 2 Batch 528 Loss 1.3376\n",
      "Epoch 2 Batch 529 Loss 1.5956\n",
      "Epoch 2 Batch 530 Loss 1.3106\n",
      "Epoch 2 Batch 531 Loss 1.4453\n",
      "Epoch 2 Batch 532 Loss 1.2756\n",
      "Epoch 2 Batch 533 Loss 1.3516\n",
      "Epoch 2 Batch 534 Loss 1.3362\n",
      "Epoch 2 Batch 535 Loss 1.3914\n",
      "Epoch 2 Batch 536 Loss 1.0699\n",
      "Epoch 2 Batch 537 Loss 1.2945\n",
      "Epoch 2 Batch 538 Loss 1.0489\n",
      "Epoch 2 Batch 539 Loss 1.5557\n",
      "Epoch 2 Batch 540 Loss 1.4528\n",
      "Epoch 2 Batch 541 Loss 1.4234\n",
      "Epoch 2 Batch 542 Loss 1.3570\n",
      "Epoch 2 Batch 543 Loss 1.2515\n",
      "Epoch 2 Batch 544 Loss 1.3710\n",
      "Epoch 2 Batch 545 Loss 1.4104\n",
      "Epoch 2 Batch 546 Loss 1.3652\n",
      "Epoch 2 Batch 547 Loss 1.3459\n",
      "Epoch 2 Batch 548 Loss 1.5373\n",
      "Epoch 2 Batch 549 Loss 1.5953\n",
      "Epoch 2 Batch 550 Loss 1.3612\n",
      "Epoch 2 Batch 551 Loss 1.5872\n",
      "Epoch 2 Batch 552 Loss 1.3025\n",
      "Epoch 2 Batch 553 Loss 1.3840\n",
      "Epoch 2 Batch 554 Loss 1.4799\n",
      "Epoch 2 Batch 555 Loss 1.7049\n",
      "Epoch 2 Batch 556 Loss 1.4468\n",
      "Epoch 2 Batch 557 Loss 1.4915\n",
      "Epoch 2 Batch 558 Loss 1.3360\n",
      "Epoch 2 Batch 559 Loss 1.3909\n",
      "Epoch 2 Batch 560 Loss 1.3991\n",
      "Epoch 2 Batch 561 Loss 1.4431\n",
      "Epoch 2 Batch 562 Loss 1.4285\n",
      "Epoch 2 Batch 563 Loss 1.5544\n",
      "Epoch 2 Batch 564 Loss 1.4911\n",
      "Epoch 2 Batch 565 Loss 1.2910\n",
      "Epoch 2 Batch 566 Loss 1.4610\n",
      "Epoch 2 Batch 567 Loss 1.4856\n",
      "Epoch 2 Batch 568 Loss 1.3995\n",
      "Epoch 2 Batch 569 Loss 1.3753\n",
      "Epoch 2 Batch 570 Loss 1.3734\n",
      "Epoch 2 Batch 571 Loss 1.3439\n",
      "Epoch 2 Batch 572 Loss 1.5769\n",
      "Epoch 2 Batch 573 Loss 1.1569\n",
      "Epoch 2 Batch 574 Loss 1.3183\n",
      "Epoch 2 Batch 575 Loss 1.2398\n",
      "Epoch 2 Batch 576 Loss 1.3205\n",
      "Epoch 2 Batch 577 Loss 1.3996\n",
      "Epoch 2 Batch 578 Loss 1.5833\n",
      "Epoch 2 Batch 579 Loss 1.6148\n",
      "Epoch 2 Batch 580 Loss 1.4507\n",
      "Epoch 2 Batch 581 Loss 1.3177\n",
      "Epoch 2 Batch 582 Loss 1.1407\n",
      "Epoch 2 Batch 583 Loss 1.5288\n",
      "Epoch 2 Batch 584 Loss 1.6227\n",
      "Epoch 2 Batch 585 Loss 1.3719\n",
      "Epoch 2 Batch 586 Loss 1.4280\n",
      "Epoch 2 Batch 587 Loss 1.5393\n",
      "Epoch 2 Batch 588 Loss 1.5185\n",
      "Epoch 2 Batch 589 Loss 1.5755\n",
      "Epoch 2 Batch 590 Loss 1.2606\n",
      "Epoch 2 Batch 591 Loss 1.3105\n",
      "Epoch 2 Batch 592 Loss 1.2609\n",
      "Epoch 2 Batch 593 Loss 1.2760\n",
      "Epoch 2 Batch 594 Loss 1.2979\n",
      "Epoch 2 Batch 595 Loss 1.5004\n",
      "Epoch 2 Batch 596 Loss 1.7711\n",
      "Epoch 2 Batch 597 Loss 1.3506\n",
      "Epoch 2 Batch 598 Loss 1.5386\n",
      "Epoch 2 Batch 599 Loss 1.6211\n",
      "Epoch 2 Batch 600 Loss 1.4591\n",
      "Epoch 2 Batch 601 Loss 1.1315\n",
      "Epoch 2 Batch 602 Loss 1.4533\n",
      "Epoch 2 Batch 603 Loss 1.4667\n",
      "Epoch 2 Batch 604 Loss 1.6044\n",
      "Epoch 2 Batch 605 Loss 1.2407\n",
      "Epoch 2 Batch 606 Loss 1.4123\n",
      "Epoch 2 Batch 607 Loss 1.3522\n",
      "Epoch 2 Batch 608 Loss 1.3698\n",
      "Epoch 2 Batch 609 Loss 1.4559\n",
      "Epoch 2 Batch 610 Loss 1.4668\n",
      "Epoch 2 Batch 611 Loss 1.6023\n",
      "Epoch 2 Batch 612 Loss 1.3034\n",
      "Epoch 2 Batch 613 Loss 1.3559\n",
      "Epoch 2 Batch 614 Loss 1.5138\n",
      "Epoch 2 Batch 615 Loss 1.2529\n",
      "Epoch 2 Batch 616 Loss 1.4042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 617 Loss 1.4985\n",
      "Epoch 2 Batch 618 Loss 1.3390\n",
      "Epoch 2 Batch 619 Loss 1.5194\n",
      "Epoch 2 Batch 620 Loss 1.5453\n",
      "Epoch 2 Batch 621 Loss 1.3329\n",
      "Epoch 2 Batch 622 Loss 1.4236\n",
      "Epoch 2 Batch 623 Loss 1.4522\n",
      "Epoch 2 Batch 624 Loss 1.2519\n",
      "Epoch 2 Batch 625 Loss 1.2301\n",
      "Epoch 2 Batch 626 Loss 1.3217\n",
      "Epoch 2 Batch 627 Loss 1.4426\n",
      "Epoch 2 Batch 628 Loss 1.3393\n",
      "Epoch 2 Batch 629 Loss 1.3275\n",
      "Epoch 2 Batch 630 Loss 1.4143\n",
      "Epoch 2 Batch 631 Loss 1.4289\n",
      "Epoch 2 Batch 632 Loss 1.3097\n",
      "Epoch 2 Batch 633 Loss 1.2700\n",
      "Epoch 2 Batch 634 Loss 1.4299\n",
      "Epoch 2 Batch 635 Loss 1.2613\n",
      "Epoch 2 Batch 636 Loss 1.1520\n",
      "Epoch 2 Batch 637 Loss 1.2885\n",
      "Epoch 2 Batch 638 Loss 1.4107\n",
      "Epoch 2 Batch 639 Loss 1.3984\n",
      "Epoch 2 Batch 640 Loss 1.2860\n",
      "Epoch 2 Batch 641 Loss 1.4820\n",
      "Epoch 2 Batch 642 Loss 1.2159\n",
      "Epoch 2 Batch 643 Loss 1.3710\n",
      "Epoch 2 Batch 644 Loss 1.5750\n",
      "Epoch 2 Batch 645 Loss 1.3816\n",
      "Epoch 2 Batch 646 Loss 1.3348\n",
      "Epoch 2 Batch 647 Loss 1.2882\n",
      "Epoch 2 Batch 648 Loss 1.5996\n",
      "Epoch 2 Batch 649 Loss 1.4469\n",
      "Epoch 2 Batch 650 Loss 1.3686\n",
      "Epoch 2 Batch 651 Loss 1.1989\n",
      "Epoch 2 Batch 652 Loss 1.4092\n",
      "Epoch 2 Batch 653 Loss 1.4170\n",
      "Epoch 2 Batch 654 Loss 1.2083\n",
      "Epoch 2 Batch 655 Loss 1.2964\n",
      "Epoch 2 Batch 656 Loss 1.3064\n",
      "Epoch 2 Batch 657 Loss 1.2001\n",
      "Epoch 2 Batch 658 Loss 1.2643\n",
      "Epoch 2 Batch 659 Loss 1.4541\n",
      "Epoch 2 Batch 660 Loss 1.4210\n",
      "Epoch 2 Batch 661 Loss 1.2830\n",
      "Epoch 2 Batch 662 Loss 1.2349\n",
      "Epoch 2 Batch 663 Loss 1.4869\n",
      "Epoch 2 Batch 664 Loss 1.3241\n",
      "Epoch 2 Batch 665 Loss 1.4250\n",
      "Epoch 2 Batch 666 Loss 1.4472\n",
      "Epoch 2 Batch 667 Loss 1.5583\n",
      "Epoch 2 Batch 668 Loss 1.3805\n",
      "Epoch 2 Batch 669 Loss 1.5825\n",
      "Epoch 2 Batch 670 Loss 1.4614\n",
      "Epoch 2 Batch 671 Loss 1.3110\n",
      "Epoch 2 Batch 672 Loss 1.4650\n",
      "Epoch 2 Batch 673 Loss 1.3919\n",
      "Epoch 2 Batch 674 Loss 1.3866\n",
      "Epoch 2 Batch 675 Loss 1.4150\n",
      "Epoch 2 Batch 676 Loss 1.4770\n",
      "Epoch 2 Batch 677 Loss 1.3254\n",
      "Epoch 2 Batch 678 Loss 1.4572\n",
      "Epoch 2 Batch 679 Loss 1.2867\n",
      "Epoch 2 Batch 680 Loss 1.3876\n",
      "Epoch 2 Batch 681 Loss 1.3763\n",
      "Epoch 2 Batch 682 Loss 1.4240\n",
      "Epoch 2 Batch 683 Loss 1.4865\n",
      "Epoch 2 Batch 684 Loss 1.3004\n",
      "Epoch 2 Batch 685 Loss 1.5321\n",
      "Epoch 2 Batch 686 Loss 1.6327\n",
      "Epoch 2 Batch 687 Loss 1.4252\n",
      "Epoch 2 Batch 688 Loss 1.5395\n"
     ]
    }
   ],
   "source": [
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken for 1 epoch 524.4936063289642 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,inputs):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_output, enc_hidden = model.encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([vocab['<START>']], 0)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        \n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        \n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += reverse_vocab[predicted_id] + ' '\n",
    "        if reverse_vocab[predicted_id] == '<STOP>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence,max_length_inp,vocab)\n",
    "    \n",
    "    result, sentence, attention_plot = evaluate(model,sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='北京 汽车 BJ 20 自动挡 最低 配 <UNK> 速 续航 技师说'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 北京 汽车 BJ 20 自动挡 最低 配 <UNK> 速 续航 技师说\n",
      "Predicted translation: 这款 车 <STOP> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(inps):\n",
    "    # 判断输入长度\n",
    "    batch_size=len(inps)\n",
    "    # 开辟结果存储list\n",
    "    preidicts=[''] * batch_size\n",
    "    \n",
    "    inps = tf.convert_to_tensor(inps)\n",
    "    # 0. 初始化隐藏层输入\n",
    "    hidden = [tf.zeros((batch_size, units))]\n",
    "    # 1. 构建encoder\n",
    "    enc_output, enc_hidden = model.encoder(inps, hidden)\n",
    "    # 2. 复制\n",
    "    dec_hidden = enc_hidden\n",
    "    # 3. <START> * BATCH_SIZE \n",
    "    dec_input = tf.expand_dims([vocab['<START>']] * batch_size, 1)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(max_length_targ):\n",
    "        # 计算上下文\n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        # 单步预测\n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "        \n",
    "        # id转换 贪婪搜索\n",
    "        predicted_ids = tf.argmax(predictions,axis=1).numpy()\n",
    "        \n",
    "        \n",
    "        for index,predicted_id in enumerate(predicted_ids):\n",
    "            preidicts[index]+= reverse_vocab[predicted_id] + ' '\n",
    "        \n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "\n",
    "    results=[]\n",
    "    for preidict in preidicts:\n",
    "        # 去掉句子前后空格\n",
    "        preidict=preidict.strip()\n",
    "        # 句子小于max len就结束了 截断\n",
    "        if '<STOP>' in preidict:\n",
    "            # 截断stop\n",
    "            preidict=preidict[:preidict.index('<STOP>')]\n",
    "        # 保存结果\n",
    "        results.append(preidict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(data_X,batch_size):\n",
    "    # 存储结果\n",
    "    results=[]\n",
    "    # 样本数量\n",
    "    sample_size=len(data_X)\n",
    "    # batch 操作轮数 math.ceil向上取整 小数 +1\n",
    "    # 因为最后一个batch可能不足一个batch size 大小 ,但是依然需要计算  \n",
    "    steps_epoch = math.ceil(sample_size/batch_size)\n",
    "    # [0,steps_epoch)\n",
    "    for i in tqdm(range(steps_epoch)):\n",
    "        batch_data = data_X[i*batch_size:(i+1)*batch_size]\n",
    "        results+=batch_predict(batch_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/313 [00:00<01:55,  2.70it/s]\u001b[A\n",
      "  1%|          | 2/313 [00:00<01:53,  2.74it/s]\u001b[A\n",
      "  1%|          | 3/313 [00:01<01:51,  2.78it/s]\u001b[A\n",
      "  1%|▏         | 4/313 [00:01<01:50,  2.79it/s]\u001b[A\n",
      "  2%|▏         | 5/313 [00:01<01:50,  2.80it/s]\u001b[A\n",
      "  2%|▏         | 6/313 [00:02<01:49,  2.81it/s]\u001b[A\n",
      "  2%|▏         | 7/313 [00:02<01:48,  2.81it/s]\u001b[A\n",
      "  3%|▎         | 8/313 [00:02<01:48,  2.81it/s]\u001b[A\n",
      "  3%|▎         | 9/313 [00:03<01:48,  2.81it/s]\u001b[A\n",
      "  3%|▎         | 10/313 [00:03<01:47,  2.81it/s]\u001b[A\n",
      "  4%|▎         | 11/313 [00:03<01:47,  2.82it/s]\u001b[A\n",
      "  4%|▍         | 12/313 [00:04<01:46,  2.82it/s]\u001b[A\n",
      "  4%|▍         | 13/313 [00:04<01:45,  2.83it/s]\u001b[A\n",
      "  4%|▍         | 14/313 [00:04<01:45,  2.85it/s]\u001b[A\n",
      "  5%|▍         | 15/313 [00:05<01:44,  2.86it/s]\u001b[A\n",
      "  5%|▌         | 16/313 [00:05<01:43,  2.86it/s]\u001b[A\n",
      "  5%|▌         | 17/313 [00:06<01:43,  2.87it/s]\u001b[A\n",
      "  6%|▌         | 18/313 [00:06<01:43,  2.85it/s]\u001b[A\n",
      "  6%|▌         | 19/313 [00:06<01:43,  2.85it/s]\u001b[A\n",
      "  6%|▋         | 20/313 [00:07<01:43,  2.84it/s]\u001b[A\n",
      "  7%|▋         | 21/313 [00:07<01:42,  2.84it/s]\u001b[A\n",
      "  7%|▋         | 22/313 [00:07<01:42,  2.84it/s]\u001b[A\n",
      "  7%|▋         | 23/313 [00:08<01:42,  2.83it/s]\u001b[A\n",
      "  8%|▊         | 24/313 [00:08<01:41,  2.85it/s]\u001b[A\n",
      "  8%|▊         | 25/313 [00:08<01:41,  2.84it/s]\u001b[A\n",
      "  8%|▊         | 26/313 [00:09<01:41,  2.84it/s]\u001b[A\n",
      "  9%|▊         | 27/313 [00:09<01:40,  2.84it/s]\u001b[A\n",
      "  9%|▉         | 28/313 [00:09<01:39,  2.85it/s]\u001b[A\n",
      "  9%|▉         | 29/313 [00:10<01:39,  2.86it/s]\u001b[A\n",
      " 10%|▉         | 30/313 [00:10<01:38,  2.86it/s]\u001b[A\n",
      " 10%|▉         | 31/313 [00:10<01:38,  2.86it/s]\u001b[A\n",
      " 10%|█         | 32/313 [00:11<01:37,  2.87it/s]\u001b[A\n",
      " 11%|█         | 33/313 [00:11<01:37,  2.87it/s]\u001b[A\n",
      " 11%|█         | 34/313 [00:11<01:37,  2.87it/s]\u001b[A\n",
      " 11%|█         | 35/313 [00:12<01:36,  2.87it/s]\u001b[A\n",
      " 12%|█▏        | 36/313 [00:12<01:36,  2.87it/s]\u001b[A\n",
      " 12%|█▏        | 37/313 [00:13<01:36,  2.85it/s]\u001b[A\n",
      " 12%|█▏        | 38/313 [00:13<01:36,  2.84it/s]\u001b[A\n",
      " 12%|█▏        | 39/313 [00:13<01:36,  2.83it/s]\u001b[A\n",
      " 13%|█▎        | 40/313 [00:14<01:36,  2.82it/s]\u001b[A\n",
      " 13%|█▎        | 41/313 [00:14<01:36,  2.83it/s]\u001b[A\n",
      " 13%|█▎        | 42/313 [00:14<01:35,  2.84it/s]\u001b[A\n",
      " 14%|█▎        | 43/313 [00:15<01:34,  2.85it/s]\u001b[A\n",
      " 14%|█▍        | 44/313 [00:15<01:34,  2.85it/s]\u001b[A\n",
      " 14%|█▍        | 45/313 [00:15<01:33,  2.86it/s]\u001b[A\n",
      " 15%|█▍        | 46/313 [00:16<01:33,  2.86it/s]\u001b[A\n",
      " 15%|█▌        | 47/313 [00:16<01:32,  2.87it/s]\u001b[A\n",
      " 15%|█▌        | 48/313 [00:16<01:32,  2.87it/s]\u001b[A\n",
      " 16%|█▌        | 49/313 [00:17<01:32,  2.87it/s]\u001b[A\n",
      " 16%|█▌        | 50/313 [00:17<01:31,  2.87it/s]\u001b[A\n",
      " 16%|█▋        | 51/313 [00:17<01:31,  2.87it/s]\u001b[A\n",
      " 17%|█▋        | 52/313 [00:18<01:31,  2.86it/s]\u001b[A\n",
      " 17%|█▋        | 53/313 [00:18<01:30,  2.86it/s]\u001b[A\n",
      " 17%|█▋        | 54/313 [00:18<01:30,  2.85it/s]\u001b[A\n",
      " 18%|█▊        | 55/313 [00:19<01:30,  2.84it/s]\u001b[A\n",
      " 18%|█▊        | 56/313 [00:19<01:30,  2.83it/s]\u001b[A\n",
      " 18%|█▊        | 57/313 [00:20<01:30,  2.83it/s]\u001b[A\n",
      " 19%|█▊        | 58/313 [00:20<01:29,  2.85it/s]\u001b[A\n",
      " 19%|█▉        | 59/313 [00:20<01:29,  2.85it/s]\u001b[A\n",
      " 19%|█▉        | 60/313 [00:21<01:28,  2.85it/s]\u001b[A\n",
      " 19%|█▉        | 61/313 [00:21<01:28,  2.85it/s]\u001b[A\n",
      " 20%|█▉        | 62/313 [00:21<01:28,  2.84it/s]\u001b[A\n",
      " 20%|██        | 63/313 [00:22<01:28,  2.83it/s]\u001b[A\n",
      " 20%|██        | 64/313 [00:22<01:27,  2.84it/s]\u001b[A\n",
      " 21%|██        | 65/313 [00:22<01:27,  2.84it/s]\u001b[A\n",
      " 21%|██        | 66/313 [00:23<01:26,  2.85it/s]\u001b[A\n",
      " 21%|██▏       | 67/313 [00:23<01:26,  2.85it/s]\u001b[A\n",
      " 22%|██▏       | 68/313 [00:23<01:26,  2.84it/s]\u001b[A\n",
      " 22%|██▏       | 69/313 [00:24<01:25,  2.84it/s]\u001b[A\n",
      " 22%|██▏       | 70/313 [00:24<01:25,  2.84it/s]\u001b[A\n",
      " 23%|██▎       | 71/313 [00:24<01:25,  2.84it/s]\u001b[A\n",
      " 23%|██▎       | 72/313 [00:25<01:24,  2.84it/s]\u001b[A\n",
      " 23%|██▎       | 73/313 [00:25<01:24,  2.83it/s]\u001b[A\n",
      " 24%|██▎       | 74/313 [00:26<01:23,  2.85it/s]\u001b[A\n",
      " 24%|██▍       | 75/313 [00:26<01:23,  2.84it/s]\u001b[A\n",
      " 24%|██▍       | 76/313 [00:26<01:23,  2.83it/s]\u001b[A\n",
      " 25%|██▍       | 77/313 [00:27<01:23,  2.84it/s]\u001b[A\n",
      " 25%|██▍       | 78/313 [00:27<01:22,  2.85it/s]\u001b[A\n",
      " 25%|██▌       | 79/313 [00:27<01:21,  2.86it/s]\u001b[A\n",
      " 26%|██▌       | 80/313 [00:28<01:21,  2.86it/s]\u001b[A\n",
      " 26%|██▌       | 81/313 [00:28<01:20,  2.87it/s]\u001b[A\n",
      " 26%|██▌       | 82/313 [00:28<01:20,  2.87it/s]\u001b[A\n",
      " 27%|██▋       | 83/313 [00:29<01:20,  2.87it/s]\u001b[A\n",
      " 27%|██▋       | 84/313 [00:29<01:19,  2.87it/s]\u001b[A\n",
      " 27%|██▋       | 85/313 [00:29<01:19,  2.87it/s]\u001b[A\n",
      " 27%|██▋       | 86/313 [00:30<01:19,  2.87it/s]\u001b[A\n",
      " 28%|██▊       | 87/313 [00:30<01:18,  2.87it/s]\u001b[A\n",
      " 28%|██▊       | 88/313 [00:30<01:18,  2.87it/s]\u001b[A\n",
      " 28%|██▊       | 89/313 [00:31<01:17,  2.87it/s]\u001b[A\n",
      " 29%|██▉       | 90/313 [00:31<01:17,  2.87it/s]\u001b[A\n",
      " 29%|██▉       | 91/313 [00:31<01:17,  2.87it/s]\u001b[A\n",
      " 29%|██▉       | 92/313 [00:32<01:17,  2.86it/s]\u001b[A\n",
      " 30%|██▉       | 93/313 [00:32<01:16,  2.86it/s]\u001b[A\n",
      " 30%|███       | 94/313 [00:33<01:16,  2.85it/s]\u001b[A\n",
      " 30%|███       | 95/313 [00:33<01:16,  2.86it/s]\u001b[A\n",
      " 31%|███       | 96/313 [00:33<01:15,  2.86it/s]\u001b[A\n",
      " 31%|███       | 97/313 [00:34<01:15,  2.86it/s]\u001b[A\n",
      " 31%|███▏      | 98/313 [00:34<01:15,  2.86it/s]\u001b[A\n",
      " 32%|███▏      | 99/313 [00:34<01:15,  2.85it/s]\u001b[A\n",
      " 32%|███▏      | 100/313 [00:35<01:14,  2.85it/s]\u001b[A\n",
      " 32%|███▏      | 101/313 [00:35<01:14,  2.84it/s]\u001b[A\n",
      " 33%|███▎      | 102/313 [00:35<01:14,  2.84it/s]\u001b[A\n",
      " 33%|███▎      | 103/313 [00:36<01:14,  2.83it/s]\u001b[A\n",
      " 33%|███▎      | 104/313 [00:36<01:14,  2.82it/s]\u001b[A\n",
      " 34%|███▎      | 105/313 [00:36<01:13,  2.82it/s]\u001b[A\n",
      " 34%|███▍      | 106/313 [00:37<01:13,  2.82it/s]\u001b[A\n",
      " 34%|███▍      | 107/313 [00:37<01:12,  2.83it/s]\u001b[A\n",
      " 35%|███▍      | 108/313 [00:37<01:12,  2.83it/s]\u001b[A\n",
      " 35%|███▍      | 109/313 [00:38<01:12,  2.83it/s]\u001b[A\n",
      " 35%|███▌      | 110/313 [00:38<01:11,  2.83it/s]\u001b[A\n",
      " 35%|███▌      | 111/313 [00:38<01:11,  2.84it/s]\u001b[A\n",
      " 36%|███▌      | 112/313 [00:39<01:10,  2.84it/s]\u001b[A\n",
      " 36%|███▌      | 113/313 [00:39<01:10,  2.83it/s]\u001b[A\n",
      " 36%|███▋      | 114/313 [00:40<01:09,  2.85it/s]\u001b[A\n",
      " 37%|███▋      | 115/313 [00:40<01:09,  2.85it/s]\u001b[A\n",
      " 37%|███▋      | 116/313 [00:40<01:08,  2.86it/s]\u001b[A\n",
      " 37%|███▋      | 117/313 [00:41<01:08,  2.84it/s]\u001b[A\n",
      " 38%|███▊      | 118/313 [00:41<01:08,  2.83it/s]\u001b[A\n",
      " 38%|███▊      | 119/313 [00:41<01:08,  2.83it/s]\u001b[A\n",
      " 38%|███▊      | 120/313 [00:42<01:08,  2.82it/s]\u001b[A\n",
      " 39%|███▊      | 121/313 [00:42<01:08,  2.82it/s]\u001b[A\n",
      " 39%|███▉      | 122/313 [00:42<01:07,  2.82it/s]\u001b[A\n",
      " 39%|███▉      | 123/313 [00:43<01:07,  2.81it/s]\u001b[A\n",
      " 40%|███▉      | 124/313 [00:43<01:07,  2.82it/s]\u001b[A\n",
      " 40%|███▉      | 125/313 [00:43<01:06,  2.83it/s]\u001b[A\n",
      " 40%|████      | 126/313 [00:44<01:05,  2.84it/s]\u001b[A\n",
      " 41%|████      | 127/313 [00:44<01:05,  2.85it/s]\u001b[A\n",
      " 41%|████      | 128/313 [00:44<01:04,  2.85it/s]\u001b[A\n",
      " 41%|████      | 129/313 [00:45<01:04,  2.84it/s]\u001b[A\n",
      " 42%|████▏     | 130/313 [00:45<01:04,  2.83it/s]\u001b[A\n",
      " 42%|████▏     | 131/313 [00:46<01:04,  2.82it/s]\u001b[A\n",
      " 42%|████▏     | 132/313 [00:46<01:04,  2.82it/s]\u001b[A\n",
      " 42%|████▏     | 133/313 [00:46<01:03,  2.81it/s]\u001b[A\n",
      " 43%|████▎     | 134/313 [00:47<01:03,  2.83it/s]\u001b[A\n",
      " 43%|████▎     | 135/313 [00:47<01:02,  2.84it/s]\u001b[A\n",
      " 43%|████▎     | 136/313 [00:47<01:02,  2.85it/s]\u001b[A\n",
      " 44%|████▍     | 137/313 [00:48<01:01,  2.86it/s]\u001b[A\n",
      " 44%|████▍     | 138/313 [00:48<01:01,  2.86it/s]\u001b[A\n",
      " 44%|████▍     | 139/313 [00:48<01:00,  2.86it/s]\u001b[A\n",
      " 45%|████▍     | 140/313 [00:49<01:00,  2.87it/s]\u001b[A\n",
      " 45%|████▌     | 141/313 [00:49<00:59,  2.87it/s]\u001b[A\n",
      " 45%|████▌     | 142/313 [00:49<00:59,  2.87it/s]\u001b[A\n",
      " 46%|████▌     | 143/313 [00:50<00:59,  2.87it/s]\u001b[A\n",
      " 46%|████▌     | 144/313 [00:50<00:58,  2.87it/s]\u001b[A\n",
      " 46%|████▋     | 145/313 [00:50<00:58,  2.87it/s]\u001b[A\n",
      " 47%|████▋     | 146/313 [00:51<00:58,  2.86it/s]\u001b[A\n",
      " 47%|████▋     | 147/313 [00:51<00:58,  2.85it/s]\u001b[A\n",
      " 47%|████▋     | 148/313 [00:52<00:57,  2.85it/s]\u001b[A\n",
      " 48%|████▊     | 149/313 [00:52<00:57,  2.85it/s]\u001b[A\n",
      " 48%|████▊     | 150/313 [00:52<00:57,  2.85it/s]\u001b[A\n",
      " 48%|████▊     | 151/313 [00:53<00:56,  2.84it/s]\u001b[A\n",
      " 49%|████▊     | 152/313 [00:53<00:56,  2.84it/s]\u001b[A\n",
      " 49%|████▉     | 153/313 [00:53<00:56,  2.84it/s]\u001b[A\n",
      " 49%|████▉     | 154/313 [00:54<00:56,  2.84it/s]\u001b[A\n",
      " 50%|████▉     | 155/313 [00:54<00:55,  2.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 156/313 [00:54<00:55,  2.83it/s]\u001b[A\n",
      " 50%|█████     | 157/313 [00:55<00:55,  2.83it/s]\u001b[A\n",
      " 50%|█████     | 158/313 [00:55<00:54,  2.83it/s]\u001b[A\n",
      " 51%|█████     | 159/313 [00:55<00:54,  2.83it/s]\u001b[A\n",
      " 51%|█████     | 160/313 [00:56<00:54,  2.83it/s]\u001b[A\n",
      " 51%|█████▏    | 161/313 [00:56<00:53,  2.83it/s]\u001b[A\n",
      " 52%|█████▏    | 162/313 [00:56<00:53,  2.83it/s]\u001b[A\n",
      " 52%|█████▏    | 163/313 [00:57<00:53,  2.83it/s]\u001b[A\n",
      " 52%|█████▏    | 164/313 [00:57<00:52,  2.83it/s]\u001b[A\n",
      " 53%|█████▎    | 165/313 [00:58<00:52,  2.83it/s]\u001b[A\n",
      " 53%|█████▎    | 166/313 [00:58<00:51,  2.83it/s]\u001b[A\n",
      " 53%|█████▎    | 167/313 [00:58<00:51,  2.82it/s]\u001b[A\n",
      " 54%|█████▎    | 168/313 [00:59<00:51,  2.82it/s]\u001b[A\n",
      " 54%|█████▍    | 169/313 [00:59<00:51,  2.81it/s]\u001b[A\n",
      " 54%|█████▍    | 170/313 [00:59<00:50,  2.81it/s]\u001b[A\n",
      " 55%|█████▍    | 171/313 [01:00<00:50,  2.81it/s]\u001b[A\n",
      " 55%|█████▍    | 172/313 [01:00<00:50,  2.81it/s]\u001b[A\n",
      " 55%|█████▌    | 173/313 [01:00<00:49,  2.81it/s]\u001b[A\n",
      " 56%|█████▌    | 174/313 [01:01<00:49,  2.81it/s]\u001b[A\n",
      " 56%|█████▌    | 175/313 [01:01<00:49,  2.81it/s]\u001b[A\n",
      " 56%|█████▌    | 176/313 [01:01<00:48,  2.81it/s]\u001b[A\n",
      " 57%|█████▋    | 177/313 [01:02<00:48,  2.82it/s]\u001b[A\n",
      " 57%|█████▋    | 178/313 [01:02<00:47,  2.82it/s]\u001b[A\n",
      " 57%|█████▋    | 179/313 [01:02<00:47,  2.82it/s]\u001b[A\n",
      " 58%|█████▊    | 180/313 [01:03<00:47,  2.82it/s]\u001b[A\n",
      " 58%|█████▊    | 181/313 [01:03<00:46,  2.83it/s]\u001b[A\n",
      " 58%|█████▊    | 182/313 [01:04<00:46,  2.83it/s]\u001b[A\n",
      " 58%|█████▊    | 183/313 [01:04<00:45,  2.83it/s]\u001b[A\n",
      " 59%|█████▉    | 184/313 [01:04<00:45,  2.83it/s]\u001b[A\n",
      " 59%|█████▉    | 185/313 [01:05<00:45,  2.84it/s]\u001b[A\n",
      " 59%|█████▉    | 186/313 [01:05<00:44,  2.83it/s]\u001b[A\n",
      " 60%|█████▉    | 187/313 [01:05<00:44,  2.82it/s]\u001b[A\n",
      " 60%|██████    | 188/313 [01:06<00:44,  2.82it/s]\u001b[A\n",
      " 60%|██████    | 189/313 [01:06<00:43,  2.82it/s]\u001b[A\n",
      " 61%|██████    | 190/313 [01:06<00:43,  2.82it/s]\u001b[A\n",
      " 61%|██████    | 191/313 [01:07<00:43,  2.82it/s]\u001b[A\n",
      " 61%|██████▏   | 192/313 [01:07<00:42,  2.82it/s]\u001b[A\n",
      " 62%|██████▏   | 193/313 [01:07<00:42,  2.82it/s]\u001b[A\n",
      " 62%|██████▏   | 194/313 [01:08<00:42,  2.82it/s]\u001b[A\n",
      " 62%|██████▏   | 195/313 [01:08<00:41,  2.81it/s]\u001b[A\n",
      " 63%|██████▎   | 196/313 [01:09<00:41,  2.81it/s]\u001b[A\n",
      " 63%|██████▎   | 197/313 [01:09<00:41,  2.81it/s]\u001b[A\n",
      " 63%|██████▎   | 198/313 [01:09<00:40,  2.81it/s]\u001b[A\n",
      " 64%|██████▎   | 199/313 [01:10<00:40,  2.81it/s]\u001b[A\n",
      " 64%|██████▍   | 200/313 [01:10<00:40,  2.81it/s]\u001b[A\n",
      " 64%|██████▍   | 201/313 [01:10<00:39,  2.81it/s]\u001b[A\n",
      " 65%|██████▍   | 202/313 [01:11<00:39,  2.81it/s]\u001b[A\n",
      " 65%|██████▍   | 203/313 [01:11<00:39,  2.81it/s]\u001b[A\n",
      " 65%|██████▌   | 204/313 [01:11<00:38,  2.81it/s]\u001b[A\n",
      " 65%|██████▌   | 205/313 [01:12<00:38,  2.81it/s]\u001b[A\n",
      " 66%|██████▌   | 206/313 [01:12<00:38,  2.81it/s]\u001b[A\n",
      " 66%|██████▌   | 207/313 [01:12<00:37,  2.81it/s]\u001b[A\n",
      " 66%|██████▋   | 208/313 [01:13<00:37,  2.83it/s]\u001b[A\n",
      " 67%|██████▋   | 209/313 [01:13<00:36,  2.84it/s]\u001b[A\n",
      " 67%|██████▋   | 210/313 [01:13<00:36,  2.85it/s]\u001b[A\n",
      " 67%|██████▋   | 211/313 [01:14<00:35,  2.85it/s]\u001b[A\n",
      " 68%|██████▊   | 212/313 [01:14<00:35,  2.85it/s]\u001b[A\n",
      " 68%|██████▊   | 213/313 [01:15<00:35,  2.85it/s]\u001b[A\n",
      " 68%|██████▊   | 214/313 [01:15<00:34,  2.84it/s]\u001b[A\n",
      " 69%|██████▊   | 215/313 [01:15<00:34,  2.83it/s]\u001b[A\n",
      " 69%|██████▉   | 216/313 [01:16<00:34,  2.83it/s]\u001b[A\n",
      " 69%|██████▉   | 217/313 [01:16<00:33,  2.83it/s]\u001b[A\n",
      " 70%|██████▉   | 218/313 [01:16<00:33,  2.84it/s]\u001b[A\n",
      " 70%|██████▉   | 219/313 [01:17<00:32,  2.85it/s]\u001b[A\n",
      " 70%|███████   | 220/313 [01:17<00:32,  2.86it/s]\u001b[A\n",
      " 71%|███████   | 221/313 [01:17<00:32,  2.87it/s]\u001b[A\n",
      " 71%|███████   | 222/313 [01:18<00:31,  2.87it/s]\u001b[A\n",
      " 71%|███████   | 223/313 [01:18<00:31,  2.87it/s]\u001b[A\n",
      " 72%|███████▏  | 224/313 [01:18<00:31,  2.87it/s]\u001b[A\n",
      " 72%|███████▏  | 225/313 [01:19<00:30,  2.87it/s]\u001b[A\n",
      " 72%|███████▏  | 226/313 [01:19<00:30,  2.87it/s]\u001b[A\n",
      " 73%|███████▎  | 227/313 [01:19<00:29,  2.88it/s]\u001b[A\n",
      " 73%|███████▎  | 228/313 [01:20<00:29,  2.88it/s]\u001b[A\n",
      " 73%|███████▎  | 229/313 [01:20<00:29,  2.88it/s]\u001b[A\n",
      " 73%|███████▎  | 230/313 [01:20<00:28,  2.88it/s]\u001b[A\n",
      " 74%|███████▍  | 231/313 [01:21<00:28,  2.88it/s]\u001b[A\n",
      " 74%|███████▍  | 232/313 [01:21<00:28,  2.88it/s]\u001b[A\n",
      " 74%|███████▍  | 233/313 [01:22<00:27,  2.87it/s]\u001b[A\n",
      " 75%|███████▍  | 234/313 [01:22<00:27,  2.87it/s]\u001b[A\n",
      " 75%|███████▌  | 235/313 [01:22<00:27,  2.87it/s]\u001b[A\n",
      " 75%|███████▌  | 236/313 [01:23<00:26,  2.86it/s]\u001b[A\n",
      " 76%|███████▌  | 237/313 [01:23<00:26,  2.86it/s]\u001b[A\n",
      " 76%|███████▌  | 238/313 [01:23<00:26,  2.85it/s]\u001b[A\n",
      " 76%|███████▋  | 239/313 [01:24<00:26,  2.85it/s]\u001b[A\n",
      " 77%|███████▋  | 240/313 [01:24<00:25,  2.84it/s]\u001b[A\n",
      " 77%|███████▋  | 241/313 [01:24<00:25,  2.84it/s]\u001b[A\n",
      " 77%|███████▋  | 242/313 [01:25<00:25,  2.83it/s]\u001b[A\n",
      " 78%|███████▊  | 243/313 [01:25<00:24,  2.83it/s]\u001b[A\n",
      " 78%|███████▊  | 244/313 [01:25<00:24,  2.82it/s]\u001b[A\n",
      " 78%|███████▊  | 245/313 [01:26<00:24,  2.82it/s]\u001b[A\n",
      " 79%|███████▊  | 246/313 [01:26<00:23,  2.83it/s]\u001b[A\n",
      " 79%|███████▉  | 247/313 [01:26<00:23,  2.84it/s]\u001b[A\n",
      " 79%|███████▉  | 248/313 [01:27<00:22,  2.85it/s]\u001b[A\n",
      " 80%|███████▉  | 249/313 [01:27<00:22,  2.85it/s]\u001b[A\n",
      " 80%|███████▉  | 250/313 [01:27<00:22,  2.86it/s]\u001b[A\n",
      " 80%|████████  | 251/313 [01:28<00:21,  2.87it/s]\u001b[A\n",
      " 81%|████████  | 252/313 [01:28<00:21,  2.86it/s]\u001b[A\n",
      " 81%|████████  | 253/313 [01:29<00:20,  2.87it/s]\u001b[A\n",
      " 81%|████████  | 254/313 [01:29<00:20,  2.87it/s]\u001b[A\n",
      " 81%|████████▏ | 255/313 [01:29<00:20,  2.86it/s]\u001b[A\n",
      " 82%|████████▏ | 256/313 [01:30<00:19,  2.86it/s]\u001b[A\n",
      " 82%|████████▏ | 257/313 [01:30<00:19,  2.86it/s]\u001b[A\n",
      " 82%|████████▏ | 258/313 [01:30<00:19,  2.87it/s]\u001b[A\n",
      " 83%|████████▎ | 259/313 [01:31<00:18,  2.86it/s]\u001b[A\n",
      " 83%|████████▎ | 260/313 [01:31<00:18,  2.86it/s]\u001b[A\n",
      " 83%|████████▎ | 261/313 [01:31<00:18,  2.86it/s]\u001b[A\n",
      " 84%|████████▎ | 262/313 [01:32<00:17,  2.86it/s]\u001b[A\n",
      " 84%|████████▍ | 263/313 [01:32<00:17,  2.86it/s]\u001b[A\n",
      " 84%|████████▍ | 264/313 [01:32<00:17,  2.86it/s]\u001b[A\n",
      " 85%|████████▍ | 265/313 [01:33<00:16,  2.86it/s]\u001b[A\n",
      " 85%|████████▍ | 266/313 [01:33<00:16,  2.86it/s]\u001b[A\n",
      " 85%|████████▌ | 267/313 [01:33<00:16,  2.86it/s]\u001b[A\n",
      " 86%|████████▌ | 268/313 [01:34<00:15,  2.86it/s]\u001b[A\n",
      " 86%|████████▌ | 269/313 [01:34<00:15,  2.86it/s]\u001b[A\n",
      " 86%|████████▋ | 270/313 [01:34<00:15,  2.87it/s]\u001b[A\n",
      " 87%|████████▋ | 271/313 [01:35<00:14,  2.87it/s]\u001b[A\n",
      " 87%|████████▋ | 272/313 [01:35<00:14,  2.87it/s]\u001b[A\n",
      " 87%|████████▋ | 273/313 [01:36<00:13,  2.88it/s]\u001b[A\n",
      " 88%|████████▊ | 274/313 [01:36<00:13,  2.88it/s]\u001b[A\n",
      " 88%|████████▊ | 275/313 [01:36<00:13,  2.86it/s]\u001b[A\n",
      " 88%|████████▊ | 276/313 [01:37<00:12,  2.85it/s]\u001b[A\n",
      " 88%|████████▊ | 277/313 [01:37<00:12,  2.84it/s]\u001b[A\n",
      " 89%|████████▉ | 278/313 [01:37<00:12,  2.84it/s]\u001b[A\n",
      " 89%|████████▉ | 279/313 [01:38<00:11,  2.85it/s]\u001b[A\n",
      " 89%|████████▉ | 280/313 [01:38<00:11,  2.85it/s]\u001b[A\n",
      " 90%|████████▉ | 281/313 [01:38<00:11,  2.86it/s]\u001b[A\n",
      " 90%|█████████ | 282/313 [01:39<00:10,  2.86it/s]\u001b[A\n",
      " 90%|█████████ | 283/313 [01:39<00:10,  2.86it/s]\u001b[A\n",
      " 91%|█████████ | 284/313 [01:39<00:10,  2.86it/s]\u001b[A\n",
      " 91%|█████████ | 285/313 [01:40<00:09,  2.84it/s]\u001b[A\n",
      " 91%|█████████▏| 286/313 [01:40<00:09,  2.83it/s]\u001b[A\n",
      " 92%|█████████▏| 287/313 [01:40<00:09,  2.82it/s]\u001b[A\n",
      " 92%|█████████▏| 288/313 [01:41<00:08,  2.82it/s]\u001b[A\n",
      " 92%|█████████▏| 289/313 [01:41<00:08,  2.82it/s]\u001b[A\n",
      " 93%|█████████▎| 290/313 [01:42<00:08,  2.81it/s]\u001b[A\n",
      " 93%|█████████▎| 291/313 [01:42<00:07,  2.81it/s]\u001b[A\n",
      " 93%|█████████▎| 292/313 [01:42<00:07,  2.81it/s]\u001b[A\n",
      " 94%|█████████▎| 293/313 [01:43<00:07,  2.82it/s]\u001b[A\n",
      " 94%|█████████▍| 294/313 [01:43<00:06,  2.84it/s]\u001b[A\n",
      " 94%|█████████▍| 295/313 [01:43<00:06,  2.84it/s]\u001b[A\n",
      " 95%|█████████▍| 296/313 [01:44<00:05,  2.84it/s]\u001b[A\n",
      " 95%|█████████▍| 297/313 [01:44<00:05,  2.83it/s]\u001b[A\n",
      " 95%|█████████▌| 298/313 [01:44<00:05,  2.83it/s]\u001b[A\n",
      " 96%|█████████▌| 299/313 [01:45<00:04,  2.82it/s]\u001b[A\n",
      " 96%|█████████▌| 300/313 [01:45<00:04,  2.83it/s]\u001b[A\n",
      " 96%|█████████▌| 301/313 [01:45<00:04,  2.83it/s]\u001b[A\n",
      " 96%|█████████▋| 302/313 [01:46<00:03,  2.84it/s]\u001b[A\n",
      " 97%|█████████▋| 303/313 [01:46<00:03,  2.84it/s]\u001b[A\n",
      " 97%|█████████▋| 304/313 [01:46<00:03,  2.83it/s]\u001b[A\n",
      " 97%|█████████▋| 305/313 [01:47<00:02,  2.83it/s]\u001b[A\n",
      " 98%|█████████▊| 306/313 [01:47<00:02,  2.82it/s]\u001b[A\n",
      " 98%|█████████▊| 307/313 [01:48<00:02,  2.82it/s]\u001b[A\n",
      " 98%|█████████▊| 308/313 [01:48<00:01,  2.82it/s]\u001b[A\n",
      " 99%|█████████▊| 309/313 [01:48<00:01,  2.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 310/313 [01:49<00:01,  2.81it/s]\u001b[A\n",
      " 99%|█████████▉| 311/313 [01:49<00:00,  2.81it/s]\u001b[A\n",
      "100%|█████████▉| 312/313 [01:49<00:00,  2.82it/s]\u001b[A\n",
      "100%|██████████| 313/313 [01:50<00:00,  2.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 6.67 s, total: 1min 53s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results=model_predict(test_X,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'， 描述 ， 没有 问题 ， 需要 四 s 店 进行 清洗 '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>大众(进口)</td>\n",
       "      <td>高尔夫(进口)</td>\n",
       "      <td>我的帕萨特烧机油怎么办怎么办？</td>\n",
       "      <td>技师说：你好，请问你的车跑了多少公里了，如果在保修期内，可以到当地的4店里面进行检查维修。如...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>一汽-大众奥迪</td>\n",
       "      <td>奥迪A6</td>\n",
       "      <td>修一下多少钱是换还是修</td>\n",
       "      <td>技师说：你好师傅！抛光处理一下就好了！50元左右就好了，希望能够帮到你！祝你生活愉快！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>上汽大众</td>\n",
       "      <td>帕萨特</td>\n",
       "      <td>帕萨特领域    喇叭坏了  店里说方向盘里线坏了 换一根两三百不等 感觉太贵</td>\n",
       "      <td>技师说：你好，气囊油丝坏了吗，这个价格不贵。可以更换。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>南京菲亚特</td>\n",
       "      <td>派力奥</td>\n",
       "      <td>发动机漏气会有什么征兆？</td>\n",
       "      <td>技师说：你好！一：发动机没力，并伴有“啪啪”的漏气声音。二：发动机没力，并伴有排气管冒黑烟。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>东风本田</td>\n",
       "      <td>思铂睿</td>\n",
       "      <td>请问 那天右后胎扎了订，补了胎后跑高速80多开始有点抖，110时速以上抖动明显，以为是未做动...</td>\n",
       "      <td>技师说：你好师傅！可能前轮平衡快脱落或者不平衡造成的！建议前轮做一下动平衡就好了！希望能够帮...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID    Brand    Model                                           Question  \\\n",
       "0  Q1   大众(进口)  高尔夫(进口)                                    我的帕萨特烧机油怎么办怎么办？   \n",
       "1  Q2  一汽-大众奥迪     奥迪A6                                        修一下多少钱是换还是修   \n",
       "2  Q3     上汽大众      帕萨特           帕萨特领域    喇叭坏了  店里说方向盘里线坏了 换一根两三百不等 感觉太贵    \n",
       "3  Q4    南京菲亚特      派力奥                                       发动机漏气会有什么征兆？   \n",
       "4  Q5     东风本田      思铂睿  请问 那天右后胎扎了订，补了胎后跑高速80多开始有点抖，110时速以上抖动明显，以为是未做动...   \n",
       "\n",
       "                                            Dialogue  \n",
       "0  技师说：你好，请问你的车跑了多少公里了，如果在保修期内，可以到当地的4店里面进行检查维修。如...  \n",
       "1        技师说：你好师傅！抛光处理一下就好了！50元左右就好了，希望能够帮到你！祝你生活愉快！  \n",
       "2                        技师说：你好，气囊油丝坏了吗，这个价格不贵。可以更换。  \n",
       "3  技师说：你好！一：发动机没力，并伴有“啪啪”的漏气声音。二：发动机没力，并伴有排气管冒黑烟。...  \n",
       "4  技师说：你好师傅！可能前轮平衡快脱落或者不平衡造成的！建议前轮做一下动平衡就好了！希望能够帮...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入提交数据\n",
    "test_df=pd.read_csv(test_data_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_proc(sentence):\n",
    "    sentence=sentence.lstrip(' ，！。')\n",
    "    sentence=sentence.replace(' ','')\n",
    "    if sentence=='':\n",
    "        sentence='随时联系'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断是否有空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,result in enumerate(results):\n",
    "    if result=='':print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 赋值结果\n",
    "test_df['Prediction']=results\n",
    "#　提取ID和预测结果两列\n",
    "test_df=test_df[['QID','Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>， 现在 行驶 一千公里 ， 保修期 内 ， 4s店 内 4s店 进行 维修 ， 检查 机油...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>！ 描述 ， 没有 等到 回复 ， 需要 300 元 左右 ！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>， 描述 情况 分析 事故 车 ， 价格 不 贵 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>分析 诊断 电脑 读取 发动机 缸 垫 。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>， 描述 ， 这种 情况 ， 轮胎 问题 ， 轮胎 问题 ， 轮胎 问题 ， 轮胎 问题 ，...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID                                         Prediction\n",
       "0  Q1  ， 现在 行驶 一千公里 ， 保修期 内 ， 4s店 内 4s店 进行 维修 ， 检查 机油...\n",
       "1  Q2                   ！ 描述 ， 没有 等到 回复 ， 需要 300 元 左右 ！ \n",
       "2  Q3                        ， 描述 情况 分析 事故 车 ， 价格 不 贵 。 \n",
       "3  Q4                             分析 诊断 电脑 读取 发动机 缸 垫 。 \n",
       "4  Q5  ， 描述 ， 这种 情况 ， 轮胎 问题 ， 轮胎 问题 ， 轮胎 问题 ， 轮胎 问题 ，..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断是否有空值\n",
    "# for predic in test_df['Prediction']:\n",
    "#     if type(predic) != str:\n",
    "#         print(predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Prediction']=test_df['Prediction'].apply(submit_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>现在行驶一千公里，保修期内，4s店内4s店进行维修，检查机油消耗过大，更换机油消耗过大，检查...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>描述，没有等到回复，需要300元左右！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>描述情况分析事故车，价格不贵。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>分析诊断电脑读取发动机缸垫。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>描述，这种情况，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QID                                         Prediction\n",
       "0  Q1  现在行驶一千公里，保修期内，4s店内4s店进行维修，检查机油消耗过大，更换机油消耗过大，检查...\n",
       "1  Q2                                描述，没有等到回复，需要300元左右！\n",
       "2  Q3                                    描述情况分析事故车，价格不贵。\n",
       "3  Q4                                     分析诊断电脑读取发动机缸垫。\n",
       "4  Q5  描述，这种情况，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_result_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取结果存储路径\n",
    "result_save_path = get_result_filename(params[\"batch_size\"],params[\"epochs\"] , params[\"max_enc_len\"], params[\"embedding_dim\"],commit='_4_1_submit_proc_add_masks_loss_seq2seq_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果.\n",
    "test_df.to_csv(result_save_path,index=None,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/roger/kaikeba/03_lecture/code/result/2019_12_08_22_30_23_batch_size_64_epochs_5_max_length_inp_200_embedding_dim_500_4_1_submit_proc_add_masks_loss_seq2seq_code.csv'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>现在行驶一千公里，保修期内，4s店内4s店进行维修，检查机油消耗过大，更换机油消耗过大，检查...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>描述，没有等到回复，需要300元左右！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>描述情况分析事故车，价格不贵。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>分析诊断电脑读取发动机缸垫。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>描述，这种情况，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q6</td>\n",
       "      <td>描述，这种情况，描述，这种情况，描述，这种情况，描述，这种情况，描述，这种情况，描述，这种情...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q7</td>\n",
       "      <td>添加，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q8</td>\n",
       "      <td>机油灯亮，需要使用5w30机油，需要检查机油灯亮，需要使用5w30机油，需要检查机油灯亮，需...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q9</td>\n",
       "      <td>图片来看，轮胎磨损严重，这种情况，轮胎磨损严重，这种情况，轮胎磨损严重，这种情况，轮胎磨损严...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q10</td>\n",
       "      <td>这种情况可能天气凉情况下，长时间停放时间长以后，时间长以后，时间长以后，时间长以后，时间长以...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QID                                         Prediction\n",
       "0   Q1  现在行驶一千公里，保修期内，4s店内4s店进行维修，检查机油消耗过大，更换机油消耗过大，检查...\n",
       "1   Q2                                描述，没有等到回复，需要300元左右！\n",
       "2   Q3                                    描述情况分析事故车，价格不贵。\n",
       "3   Q4                                     分析诊断电脑读取发动机缸垫。\n",
       "4   Q5  描述，这种情况，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问题，轮胎问...\n",
       "5   Q6  描述，这种情况，描述，这种情况，描述，这种情况，描述，这种情况，描述，这种情况，描述，这种情...\n",
       "6   Q7  添加，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添加防冻液，添...\n",
       "7   Q8  机油灯亮，需要使用5w30机油，需要检查机油灯亮，需要使用5w30机油，需要检查机油灯亮，需...\n",
       "8   Q9  图片来看，轮胎磨损严重，这种情况，轮胎磨损严重，这种情况，轮胎磨损严重，这种情况，轮胎磨损严...\n",
       "9  Q10  这种情况可能天气凉情况下，长时间停放时间长以后，时间长以后，时间长以后，时间长以后，时间长以..."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取结果\n",
    "test_df=pd.read_csv(result_save_path)\n",
    "# 查看格式\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交须知\n",
    "请在提交之前仔细阅读“提交须知”。\n",
    "\n",
    "1. 自动评审\n",
    "系统根据选手提交的结果自动评分，提供每天5次的评测与排名机会，实时更新排行榜并按照评测分数从高到低排序。若一天内多次提交结果，新结果版本将覆盖原版本。\n",
    "\n",
    "2. 评分标准\n",
    "评测用到的核心算法为ROUGE(Recall-Oriented Understudy for Gisting Evaluation)，详见Wikipedia；具体用到的指标为ROUGE_L，即：Longest Common Subsequence (LCS) based statistics，关于LCS问题，详见Wikipedia。\n",
    "\n",
    "所有参与评审的模型必须使用飞桨PaddlePaddle。所有参赛个人可无限使用基于AI Studio平台提供的训练资源。\n",
    "\n",
    "3. 特别注意\n",
    "选手需确认输出结果的总行数为20001（含表头），且QID ≤ Q20000，否则成绩无效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 角度1：QA 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 角度2： 摘要问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 角度3： 阅读理解问题\n",
    "\n",
    "> 如果看成是阅读理解问题， 那么就是从Conversation中找出能回答Problem的答案， 由于目前的阅读理解数据集的答案长度通常比较短（一般是几个单词），所以state of the art的作法是根据Problem，从Context中选择一段作为答案，模型只要输出答案的开始和结束位置即可。 但是这个任务的report有点长，常常出现几十个甚至上百个词， 而且report中的词好像并不完全是来自于Conversation。 Report中67.7%的词来自于Conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n",
    "* [Neural Machine Translation (seq2seq) Tutorial](https://github.com/tensorflow/nmt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture02",
   "language": "python",
   "name": "lecture02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
