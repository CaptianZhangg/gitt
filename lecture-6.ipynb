{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copynet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pointer-generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.w_s_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_i_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_c_reduce = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, context_vector, state, dec_inp):\n",
    "        return tf.nn.sigmoid(self.w_s_reduce(state)+self.w_c_reduce(context_vector)+self.w_i_reduce(dec_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self, params):\n",
    "        super(PGN, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.attention = BahdanauAttention()\n",
    "        self.decoder = Decoder()\n",
    "        self.pointer = Pointer()\n",
    "    \n",
    "    def call_encoder(self, enc_inp):\n",
    "        enc_hidden = self.encoder.initialize_hidden_state()\n",
    "        enc_output, enc_hidden = self.encoder(enc_inp, enc_hidden)\n",
    "        return enc_hidden, enc_output\n",
    "    \n",
    "    def call(self, enc_output, dec_hidden, enc_inp, enc_extended_inp,  dec_inp, batch_oov_len):\n",
    "        predictions = []\n",
    "        attentions = []\n",
    "        p_gens = []\n",
    "        context_vector, _ = self.attention(dec_hidden, enc_output)\n",
    "        for t in range(dec_inp.shape[1]):\n",
    "            dec_x, pred, dec_hidden = self.decoder(tf.expand_dims(dec_inp[:, t],1), dec_hidden, enc_output, context_vector)\n",
    "            context_vector, attn = self.attention(dec_hidden, enc_output)\n",
    "            p_gen = self.pointer(context_vector, dec_hidden, tf.squeeze(dec_x, axis=1))\n",
    "      \n",
    "            predictions.append(pred)\n",
    "            attentions.append(attn)\n",
    "            p_gens.append(p_gen)\n",
    "        # 计算最终的分布\n",
    "        final_dists = _calc_final_dist()\n",
    "        return final_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coverage_loss(attn_dists, padding_mask):\n",
    "    \"\"\"\n",
    "    Calculates the coverage loss from the attention distributions.\n",
    "    \n",
    "    \"\"\"\n",
    "    coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
    "    covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "    for a in attn_dists:\n",
    "        covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "        covlosses.append(covloss)\n",
    "        coverage += a # update the coverage vector\n",
    "    coverage_loss = _mask_and_avg(covlosses, padding_mask)\n",
    "    return coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_coverage and prev_coverage is not None:\n",
    "    # self.W_s(values) [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "    # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]\n",
    "    score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "    # attention_weights shape (batch_size, max_len, 1)\n",
    "    mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "    masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "    masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "    attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "\n",
    "    coverage = attention_weights + prev_coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    calculate Pgen\n",
    "    input context_vector [batch_sz,enc_units] dec_hidden [batch_sz,dec_units] dec_inp_context [batch_sz,1,embedding_dim+enc_units]\n",
    "    output scaler pgen\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.w_s_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_i_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_c_reduce = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, context_vector, dec_hidden, dec_inp):\n",
    "        # change dec_inp_context to [batch_sz,embedding_dim+enc_units]\n",
    "        dec_inp = tf.squeeze(dec_inp, axis=1)\n",
    "        pgen = tf.nn.sigmoid(self.w_s_reduce(dec_hidden) + self.w_c_reduce(context_vector) + self.w_i_reduce(dec_inp))\n",
    "        return pgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先验知识其他技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleShift(Layer):\n",
    "    \"\"\"缩放平移变换层（Scale and shift）\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaleShift, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape = (1,) * (len(input_shape)-1) + (input_shape[-1],)\n",
    "        self.log_scale = self.add_weight(name='log_scale',\n",
    "                                         shape=kernel_shape,\n",
    "                                         initializer='zeros')\n",
    "        self.shift = self.add_weight(name='shift',\n",
    "                                     shape=kernel_shape,\n",
    "                                     initializer='zeros')\n",
    "    def call(self, inputs):\n",
    "        x_outs = K.exp(self.log_scale) * inputs + self.shift\n",
    "        return x_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN（lstm/gru)参数初始化\n",
    "\n",
    "- 梯度消减\n",
    "\n",
    "- Dropout/L2 regularization\n",
    "\n",
    "- learning rate\n",
    "\n",
    "- 双向lstm、gru\n",
    "\n",
    "- 数据预处理\n",
    "\n",
    "- encoder和decoder的embedding共享参数（也就是用一套词向量）\n",
    "\n",
    "- 采用预训练词向量\n",
    "\n",
    "- batch size调小，16，32，64\n",
    "\n",
    "- hidden states 128，256\n",
    "\n",
    "- Adam betra1=0.9， betra2=0.999， e=10-8\n",
    "\n",
    "- learning rate=0.0001， 0.001\n",
    "\n",
    "- clipping gradient = 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
